{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLM scoring CLI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM1ioA9FY5OiAH6DV1fNJ+S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8691c5082cf745479085747f9c159706": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e53402998a734b31aa31a384bef621c3",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a457e42993134ec6b110c4e53c8e32da",
              "IPY_MODEL_d291bff7c3e8427799f755d71b9e1f1a"
            ]
          }
        },
        "e53402998a734b31aa31a384bef621c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a457e42993134ec6b110c4e53c8e32da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_546b4c537cf943548858de638fe4b427",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 459,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 459,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f1aa560d01154e508208aece741b0a04"
          }
        },
        "d291bff7c3e8427799f755d71b9e1f1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d271417f728d4141a47aafbe02e51aa5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 459/459 [00:00&lt;00:00, 2.16kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d7175630ee9d4380977f8da1d6342c40"
          }
        },
        "546b4c537cf943548858de638fe4b427": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f1aa560d01154e508208aece741b0a04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d271417f728d4141a47aafbe02e51aa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d7175630ee9d4380977f8da1d6342c40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5fcf78adf1ae427b96abb34b0ad65c10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_956a77aae3fc4796a1d77b1c6a0f3ed9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_58ab39f534c844e299bc061bfabe44a9",
              "IPY_MODEL_411ff1365dc64a50b940368d1606c6c4"
            ]
          }
        },
        "956a77aae3fc4796a1d77b1c6a0f3ed9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "58ab39f534c844e299bc061bfabe44a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_cfeb1980f2ab44beac4cc53acbfd43ab",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 531146902,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 531146902,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2ccb3f0909a04f8e86f1d1018aae461e"
          }
        },
        "411ff1365dc64a50b940368d1606c6c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ff3bce3eb33b4fe98c2a7411fee035ee",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 531M/531M [00:07&lt;00:00, 71.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8853411884054f86a9c5b667dd7b8261"
          }
        },
        "cfeb1980f2ab44beac4cc53acbfd43ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2ccb3f0909a04f8e86f1d1018aae461e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ff3bce3eb33b4fe98c2a7411fee035ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8853411884054f86a9c5b667dd7b8261": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4421cb8fcee342289f3b49d1af10dc26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ef03ff23011f40b298fb6fa034e2ca69",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_474518dbb368423a9c4cff7233b06c47",
              "IPY_MODEL_ccb70e4dfa0b47598270c9efeb764b8c"
            ]
          }
        },
        "ef03ff23011f40b298fb6fa034e2ca69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "474518dbb368423a9c4cff7233b06c47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e9efb15ea91840e6bfe510fbdcee56ac",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 494801,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 494801,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cc86aa701cba4123a9d83e15e7b8f904"
          }
        },
        "ccb70e4dfa0b47598270c9efeb764b8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e1f568b7853f460daeee4894b80cbb41",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 495k/495k [00:02&lt;00:00, 247kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6831f491a2004eb3bacd2fa14b344736"
          }
        },
        "e9efb15ea91840e6bfe510fbdcee56ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cc86aa701cba4123a9d83e15e7b8f904": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e1f568b7853f460daeee4894b80cbb41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6831f491a2004eb3bacd2fa14b344736": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aee61a98db88477ea44077444bd0c61a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_60842ef7f5814d6fac9faf562ab1bdc4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4187f8897f9941be968d5648b24e612a",
              "IPY_MODEL_14db583913e248ee873d0b57427c579e"
            ]
          }
        },
        "60842ef7f5814d6fac9faf562ab1bdc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4187f8897f9941be968d5648b24e612a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6c4e4f1ab8014d8894af50e3265672b8",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 112,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 112,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_80b5e6f27524433880a7bbcf6f8a78ec"
          }
        },
        "14db583913e248ee873d0b57427c579e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fdd09070603e415c8542844e8d4c570d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 112/112 [00:00&lt;00:00, 145B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3071b4fd31bb4fb0956834afd0ffcb98"
          }
        },
        "6c4e4f1ab8014d8894af50e3265672b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "80b5e6f27524433880a7bbcf6f8a78ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fdd09070603e415c8542844e8d4c570d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3071b4fd31bb4fb0956834afd0ffcb98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7eb66019ebbe4ce7a1abcab1d6a36c87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_795e4720e0514a0ca1ef3c5ec8deda01",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d3f59f2d5527402b997de7b30078a330",
              "IPY_MODEL_80c9cafee06b49589eca672f6ad5c197"
            ]
          }
        },
        "795e4720e0514a0ca1ef3c5ec8deda01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d3f59f2d5527402b997de7b30078a330": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_47de53babab240b584a9a6331319a508",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 2,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_38fac7fb4b7f4bed94562cfbce34321b"
          }
        },
        "80c9cafee06b49589eca672f6ad5c197": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4ff9adc1150b4e888fccb584914e7517",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2.00/2.00 [00:00&lt;00:00, 22.3B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bcfef62de69e48c4af890dfef0dfa28a"
          }
        },
        "47de53babab240b584a9a6331319a508": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "38fac7fb4b7f4bed94562cfbce34321b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4ff9adc1150b4e888fccb584914e7517": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bcfef62de69e48c4af890dfef0dfa28a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jimregan/effective-eureka/blob/mlm-cli/MLM_scoring_CLI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVw2PzcmYnB0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "db033c29-a10e-426e-85c3-e2d3d9fe9b3a"
      },
      "source": [
        "!pip install -q git+https://github.com/awslabs/mlm-scoring"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 245kB 5.5MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51kB 8.0MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 645kB 12.9MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.8MB 30.8MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1MB 45.7MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890kB 61.8MB/s \n",
            "\u001b[?25h  Building wheel for mlm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for mosestokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for uctools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for toolwrapper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbxxwNVeZLoA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313,
          "referenced_widgets": [
            "8691c5082cf745479085747f9c159706",
            "e53402998a734b31aa31a384bef621c3",
            "a457e42993134ec6b110c4e53c8e32da",
            "d291bff7c3e8427799f755d71b9e1f1a",
            "546b4c537cf943548858de638fe4b427",
            "f1aa560d01154e508208aece741b0a04",
            "d271417f728d4141a47aafbe02e51aa5",
            "d7175630ee9d4380977f8da1d6342c40",
            "5fcf78adf1ae427b96abb34b0ad65c10",
            "956a77aae3fc4796a1d77b1c6a0f3ed9",
            "58ab39f534c844e299bc061bfabe44a9",
            "411ff1365dc64a50b940368d1606c6c4",
            "cfeb1980f2ab44beac4cc53acbfd43ab",
            "2ccb3f0909a04f8e86f1d1018aae461e",
            "ff3bce3eb33b4fe98c2a7411fee035ee",
            "8853411884054f86a9c5b667dd7b8261",
            "4421cb8fcee342289f3b49d1af10dc26",
            "ef03ff23011f40b298fb6fa034e2ca69",
            "474518dbb368423a9c4cff7233b06c47",
            "ccb70e4dfa0b47598270c9efeb764b8c",
            "e9efb15ea91840e6bfe510fbdcee56ac",
            "cc86aa701cba4123a9d83e15e7b8f904",
            "e1f568b7853f460daeee4894b80cbb41",
            "6831f491a2004eb3bacd2fa14b344736",
            "aee61a98db88477ea44077444bd0c61a",
            "60842ef7f5814d6fac9faf562ab1bdc4",
            "4187f8897f9941be968d5648b24e612a",
            "14db583913e248ee873d0b57427c579e",
            "6c4e4f1ab8014d8894af50e3265672b8",
            "80b5e6f27524433880a7bbcf6f8a78ec",
            "fdd09070603e415c8542844e8d4c570d",
            "3071b4fd31bb4fb0956834afd0ffcb98",
            "7eb66019ebbe4ce7a1abcab1d6a36c87",
            "795e4720e0514a0ca1ef3c5ec8deda01",
            "d3f59f2d5527402b997de7b30078a330",
            "80c9cafee06b49589eca672f6ad5c197",
            "47de53babab240b584a9a6331319a508",
            "38fac7fb4b7f4bed94562cfbce34321b",
            "4ff9adc1150b4e888fccb584914e7517",
            "bcfef62de69e48c4af890dfef0dfa28a"
          ]
        },
        "outputId": "75f01880-d9ba-477b-a16f-e4204d628a5a"
      },
      "source": [
        "!mkdir bert-base-polish-uncased-v1\n",
        "from transformers import *\n",
        "model = BertModel.from_pretrained(\"dkleczek/bert-base-polish-uncased-v1\")\n",
        "tokenizer = BertTokenizer.from_pretrained(\"dkleczek/bert-base-polish-uncased-v1\")\n",
        "# https://github.com/huggingface/transformers/issues/2157\n",
        "model.save_pretrained('./bert-base-polish-uncased-v1')\n",
        "tokenizer.save_pretrained('./bert-base-polish-uncased-v1')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8691c5082cf745479085747f9c159706",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=459.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5fcf78adf1ae427b96abb34b0ad65c10",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=531146902.0, style=ProgressStyle(descriâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4421cb8fcee342289f3b49d1af10dc26",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=494801.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aee61a98db88477ea44077444bd0c61a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=112.0, style=ProgressStyle(description_â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7eb66019ebbe4ce7a1abcab1d6a36c87",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2.0, style=ProgressStyle(description_wiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./bert-base-polish-uncased-v1/vocab.txt',\n",
              " './bert-base-polish-uncased-v1/special_tokens_map.json',\n",
              " './bert-base-polish-uncased-v1/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "py-HWpxOlpnz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "8621fda5-eb4a-4e12-cc26-a24ab5f0925d"
      },
      "source": [
        "!pip install -q mxnet-cu101"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 834.1MB 51.5MB/s eta 0:00:01tcmalloc: large alloc 1147494400 bytes == 0x39fa2000 @  0x7fa03b93d615 0x591f47 0x4cc229 0x4cc38b 0x50a51c 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50cfd6 0x58e793 0x50c467 0x58e793 0x50c467 0x58e793 0x50c467 0x58e793 0x50c467 0x509918 0x50a64d 0x50c1f4 0x507f24 0x509c50 0x50a64d 0x50c1f4 0x509918 0x50a64d\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 846.0MB 21kB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btQxaMPlodml",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2a9d2ea2-79c9-4c15-d2f8-cbed088e0e75"
      },
      "source": [
        "!wget http://poleval.pl/task1/Poleval2020Task1Eval.tar.xz"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-03 20:23:06--  http://poleval.pl/task1/Poleval2020Task1Eval.tar.xz\n",
            "Resolving poleval.pl (poleval.pl)... 213.135.36.94\n",
            "Connecting to poleval.pl (poleval.pl)|213.135.36.94|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12592244 (12M) [application/x-tar]\n",
            "Saving to: â€˜Poleval2020Task1Eval.tar.xzâ€™\n",
            "\n",
            "Poleval2020Task1Eva 100%[===================>]  12.01M  5.06MB/s    in 2.4s    \n",
            "\n",
            "2020-09-03 20:23:09 (5.06 MB/s) - â€˜Poleval2020Task1Eval.tar.xzâ€™ saved [12592244/12592244]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2Fna7siopq8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar xf Poleval2020Task1Eval.tar.xz"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yuaKTeN0Q0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cat Poleval2020Task1Eval/nbest.txt | cut -d ' ' -f2- > nb"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0vexS_qfcbF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "fca92e32-9454-424e-ba00-b3b546329972"
      },
      "source": [
        "!apt install -q libjson-perl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libcommon-sense-perl libjson-xs-perl libtypes-serialiser-perl\n",
            "The following NEW packages will be installed:\n",
            "  libcommon-sense-perl libjson-perl libjson-xs-perl libtypes-serialiser-perl\n",
            "0 upgraded, 4 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 187 kB of archives.\n",
            "After this operation, 528 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libcommon-sense-perl amd64 3.74-2build2 [20.3 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libjson-perl all 2.97001-1 [73.3 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libtypes-serialiser-perl all 1.0-1 [12.1 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libjson-xs-perl amd64 3.040-1 [81.2 kB]\n",
            "Fetched 187 kB in 1s (192 kB/s)\n",
            "Selecting previously unselected package libcommon-sense-perl.\n",
            "(Reading database ... 144579 files and directories currently installed.)\n",
            "Preparing to unpack .../libcommon-sense-perl_3.74-2build2_amd64.deb ...\n",
            "Unpacking libcommon-sense-perl (3.74-2build2) ...\n",
            "Selecting previously unselected package libjson-perl.\n",
            "Preparing to unpack .../libjson-perl_2.97001-1_all.deb ...\n",
            "Unpacking libjson-perl (2.97001-1) ...\n",
            "Selecting previously unselected package libtypes-serialiser-perl.\n",
            "Preparing to unpack .../libtypes-serialiser-perl_1.0-1_all.deb ...\n",
            "Unpacking libtypes-serialiser-perl (1.0-1) ...\n",
            "Selecting previously unselected package libjson-xs-perl.\n",
            "Preparing to unpack .../libjson-xs-perl_3.040-1_amd64.deb ...\n",
            "Unpacking libjson-xs-perl (3.040-1) ...\n",
            "Setting up libcommon-sense-perl (3.74-2build2) ...\n",
            "Setting up libtypes-serialiser-perl (1.0-1) ...\n",
            "Setting up libjson-perl (2.97001-1) ...\n",
            "Setting up libjson-xs-perl (3.040-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6aDkrD_rb_W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "762ff715-dee9-4c81-c909-23e7ff660bee"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/jimregan/effective-eureka/master/scripts/mk-hyp-json.pl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-09-03 11:47:47--  https://raw.githubusercontent.com/jimregan/effective-eureka/master/scripts/mk-hyp-json.pl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 989 [text/plain]\n",
            "Saving to: â€˜mk-hyp-json.plâ€™\n",
            "\n",
            "mk-hyp-json.pl      100%[===================>]     989  --.-KB/s    in 0s      \n",
            "\n",
            "2020-09-03 11:47:48 (66.0 MB/s) - â€˜mk-hyp-json.plâ€™ saved [989/989]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCTgVe41revs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!perl mk-hyp-json.pl Poleval2020Task1Eval/nbest.txt > nb.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1RncJAwfjim",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "outputId": "094927d5-9d05-4a28-fdd3-038d1c891baf"
      },
      "source": [
        "!mlm score --mode hyp --model bert-base-polish-uncased-v1 --max-utts 3 --gpus 0 --eos nb.json > output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 11:48:25.035145: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 11:48:35 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 11:48:35 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 11:48:35 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 11:48:35 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 11:48:54 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
            "2020-09-03 11:48:54 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 11:48:54 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 11:48:54 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 11:48:54 INFO     loading file None\n",
            "2020-09-03 11:48:54 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 11:48:54 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 11:48:54 WARNING  Language was set but this model does not use language embeddings!\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/mlm\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/mlm/cmds.py\", line 184, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/mlm/cmds.py\", line 215, in cmd_score\n",
            "    preds = Predictions.from_file(args.infile, max_utts=args.max_utts)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/mlm/loaders.py\", line 225, in from_file\n",
            "    return cls.from_dict(obj_dict, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/mlm/loaders.py\", line 270, in from_dict\n",
            "    scores[idx] = hyp_data['score']\n",
            "KeyError: 'score'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWA4RsJk0YXF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        },
        "outputId": "4adf30e1-1d64-48d9-df46-acd5688f0a96"
      },
      "source": [
        "!mlm score --mode hyp --model bert-base-polish-uncased-v1 --max-utts 3 --gpus 0 --eos nb.txt > output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 12:00:52.674136: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 12:00:59 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 12:00:59 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 12:00:59 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 12:00:59 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 12:01:18 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
            "2020-09-03 12:01:18 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 12:01:18 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 12:01:18 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 12:01:18 INFO     loading file None\n",
            "2020-09-03 12:01:18 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 12:01:18 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 12:01:19 WARNING  Language was set but this model does not use language embeddings!\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/mlm\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/mlm/cmds.py\", line 184, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/mlm/cmds.py\", line 215, in cmd_score\n",
            "    preds = Predictions.from_file(args.infile, max_utts=args.max_utts)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/mlm/loaders.py\", line 229, in from_file\n",
            "    raise ValueError(\"Hypothesis file of type '{}' is not supported\".format(suffix))\n",
            "ValueError: Hypothesis file of type '.txt' is not supported\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0_exvm38PsY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!split -l 100 nb.txt "
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dahbm64N763m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bf8522ef-377c-434a-a843-347a45968e02"
      },
      "source": [
        "!for i in x[a-z][a-z];do mlm score --mode ref --model bert-base-polish-uncased-v1 --gpus 0 --eos $i > output$i;done"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 12:30:30.465600: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 12:30:54 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 12:30:54 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 12:30:54 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 12:30:54 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 12:31:13 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
            "2020-09-03 12:31:13 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 12:31:13 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 12:31:13 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 12:31:13 INFO     loading file None\n",
            "2020-09-03 12:31:13 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 12:31:13 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 12:31:13 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 12:31:23 WARNING  # sentences: 100\n",
            "2020-09-03 12:31:23 INFO     FixedBucketSampler:\n",
            "  sample_num=13622, batch_num=29\n",
            "  key=[137, 138, 139, 140]\n",
            "  cnt=[2700, 5984, 4110, 828]\n",
            "  batch_size=[510, 507, 503, 500]\n",
            "[12:31:23] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 12:32:46 INFO     9501 sents of 13622, batch 20 of 29\n",
            "2020-09-03 12:33:24 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 12:33:24 WARNING  # words (excluding EOS '.'): 10906\n",
            "2020-09-03 12:33:24 WARNING  longest sentence: 110\n",
            "2020-09-03 12:33:24 WARNING  # tokens (including EOS '.'): 13622\n",
            "2020-09-03 12:33:24 WARNING  Token-level (P)PPL: 81412.89964474668\n",
            "2020-09-03 12:33:24 WARNING  Word-normalized (P)PPL: 1360312.3691288917\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 12:33:31.180453: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 12:33:39 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 12:33:39 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 12:33:39 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 12:33:39 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 12:33:44 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
            "2020-09-03 12:33:44 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 12:33:44 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 12:33:44 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 12:33:44 INFO     loading file None\n",
            "2020-09-03 12:33:44 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 12:33:44 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 12:33:44 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 12:33:48 WARNING  # sentences: 100\n",
            "2020-09-03 12:33:49 INFO     FixedBucketSampler:\n",
            "  sample_num=18193, batch_num=39\n",
            "  key=[182, 183, 184, 185, 186]\n",
            "  cnt=[1080, 4887, 7098, 4392, 736]\n",
            "  batch_size=[510, 508, 505, 502, 500]\n",
            "[12:33:49] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 12:35:57 INFO     9673 sents of 18193, batch 20 of 39\n",
            "2020-09-03 12:37:54 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 12:37:54 WARNING  # words (excluding EOS '.'): 13619\n",
            "2020-09-03 12:37:54 WARNING  longest sentence: 137\n",
            "2020-09-03 12:37:54 WARNING  # tokens (including EOS '.'): 18193\n",
            "2020-09-03 12:37:54 WARNING  Token-level (P)PPL: 76044.5951016086\n",
            "2020-09-03 12:37:54 WARNING  Word-normalized (P)PPL: 3314333.8296350273\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 12:37:58.150722: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 12:38:05 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 12:38:05 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 12:38:05 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 12:38:05 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 12:38:10 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "2020-09-03 12:38:10 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 12:38:10 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 12:38:10 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 12:38:10 INFO     loading file None\n",
            "2020-09-03 12:38:10 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 12:38:10 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 12:38:10 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 12:38:14 WARNING  # sentences: 100\n",
            "2020-09-03 12:38:14 INFO     FixedBucketSampler:\n",
            "  sample_num=21879, batch_num=48\n",
            "  key=[217, 218, 219, 220, 221, 222, 223]\n",
            "  cnt=[430, 1080, 2170, 5232, 4380, 7040, 1547]\n",
            "  batch_size=[513, 511, 509, 506, 504, 502, 500]\n",
            "[12:38:14] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 12:40:49 INFO     9091 sents of 21879, batch 20 of 48\n",
            "2020-09-03 12:43:36 INFO     18708 sents of 21879, batch 40 of 48\n",
            "2020-09-03 12:44:31 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 12:44:31 WARNING  # words (excluding EOS '.'): 15786\n",
            "2020-09-03 12:44:31 WARNING  longest sentence: 158\n",
            "2020-09-03 12:44:31 WARNING  # tokens (including EOS '.'): 21879\n",
            "2020-09-03 12:44:31 WARNING  Token-level (P)PPL: 84773.21285195176\n",
            "2020-09-03 12:44:31 WARNING  Word-normalized (P)PPL: 6767727.305797961\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 12:44:35.190404: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 12:44:42 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 12:44:42 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 12:44:42 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 12:44:42 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 12:44:47 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
            "2020-09-03 12:44:47 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 12:44:47 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 12:44:47 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 12:44:47 INFO     loading file None\n",
            "2020-09-03 12:44:47 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 12:44:47 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 12:44:47 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 12:44:51 WARNING  # sentences: 100\n",
            "2020-09-03 12:44:52 INFO     FixedBucketSampler:\n",
            "  sample_num=32512, batch_num=66\n",
            "  key=[326, 327, 328]\n",
            "  cnt=[4860, 18850, 8802]\n",
            "  batch_size=[503, 501, 500]\n",
            "[12:44:52] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 12:49:19 INFO     9804 sents of 32512, batch 20 of 66\n",
            "2020-09-03 12:54:00 INFO     19824 sents of 32512, batch 40 of 66\n",
            "2020-09-03 12:58:36 INFO     29664 sents of 32512, batch 60 of 66\n",
            "2020-09-03 12:59:56 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 12:59:56 WARNING  # words (excluding EOS '.'): 24630\n",
            "2020-09-03 12:59:56 WARNING  longest sentence: 246\n",
            "2020-09-03 12:59:56 WARNING  # tokens (including EOS '.'): 32512\n",
            "2020-09-03 12:59:56 WARNING  Token-level (P)PPL: 62066.039602547935\n",
            "2020-09-03 12:59:56 WARNING  Word-normalized (P)PPL: 2121510.38879432\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 12:59:59.463991: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 13:00:06 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 13:00:06 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 13:00:06 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 13:00:06 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 13:00:11 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
            "2020-09-03 13:00:11 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 13:00:11 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 13:00:11 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 13:00:11 INFO     loading file None\n",
            "2020-09-03 13:00:11 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 13:00:11 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 13:00:11 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 13:00:16 WARNING  # sentences: 100\n",
            "/usr/local/lib/python3.6/dist-packages/gluonnlp/data/sampler.py:354: UserWarning: Some buckets are empty and will be removed. Unused bucket keys=[218]\n",
            "  str(unused_bucket_keys))\n",
            "2020-09-03 13:00:16 INFO     FixedBucketSampler:\n",
            "  sample_num=21352, batch_num=44\n",
            "  key=[214, 215, 216, 217, 219]\n",
            "  cnt=[3392, 7029, 8132, 2365, 434]\n",
            "  batch_size=[511, 509, 506, 504, 500]\n",
            "[13:00:16] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 13:03:00 INFO     9883 sents of 21352, batch 20 of 44\n",
            "2020-09-03 13:05:44 INFO     19493 sents of 21352, batch 40 of 44\n",
            "2020-09-03 13:06:16 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 13:06:16 WARNING  # words (excluding EOS '.'): 16745\n",
            "2020-09-03 13:06:16 WARNING  longest sentence: 168\n",
            "2020-09-03 13:06:16 WARNING  # tokens (including EOS '.'): 21352\n",
            "2020-09-03 13:06:16 WARNING  Token-level (P)PPL: 75946.10989123839\n",
            "2020-09-03 13:06:16 WARNING  Word-normalized (P)PPL: 1672106.3975895408\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 13:06:19.517338: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 13:06:26 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 13:06:26 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 13:06:26 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 13:06:26 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 13:06:31 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
            "2020-09-03 13:06:31 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 13:06:31 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 13:06:31 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 13:06:31 INFO     loading file None\n",
            "2020-09-03 13:06:31 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 13:06:31 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 13:06:32 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 13:06:36 WARNING  # sentences: 100\n",
            "2020-09-03 13:06:36 INFO     FixedBucketSampler:\n",
            "  sample_num=15967, batch_num=33\n",
            "  key=[161, 162, 163]\n",
            "  cnt=[6837, 7520, 1610]\n",
            "  batch_size=[506, 503, 500]\n",
            "[13:06:36] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 13:08:32 INFO     9636 sents of 15967, batch 20 of 33\n",
            "2020-09-03 13:09:50 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 13:09:50 WARNING  # words (excluding EOS '.'): 13046\n",
            "2020-09-03 13:09:50 WARNING  longest sentence: 131\n",
            "2020-09-03 13:09:50 WARNING  # tokens (including EOS '.'): 15967\n",
            "2020-09-03 13:09:50 WARNING  Token-level (P)PPL: 103219.65595585534\n",
            "2020-09-03 13:09:50 WARNING  Word-normalized (P)PPL: 1368812.7464837846\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 13:09:53.769067: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 13:10:01 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 13:10:01 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 13:10:01 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 13:10:01 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 13:10:05 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
            "2020-09-03 13:10:05 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 13:10:05 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 13:10:05 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 13:10:05 INFO     loading file None\n",
            "2020-09-03 13:10:05 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 13:10:05 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 13:10:06 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 13:10:10 WARNING  # sentences: 100\n",
            "2020-09-03 13:10:10 INFO     FixedBucketSampler:\n",
            "  sample_num=18682, batch_num=39\n",
            "  key=[187, 188, 189, 190]\n",
            "  cnt=[2035, 2976, 9911, 3760]\n",
            "  batch_size=[508, 505, 502, 500]\n",
            "[13:10:10] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 13:12:29 INFO     9784 sents of 18682, batch 20 of 39\n",
            "2020-09-03 13:14:38 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 13:14:38 WARNING  # words (excluding EOS '.'): 14811\n",
            "2020-09-03 13:14:38 WARNING  longest sentence: 148\n",
            "2020-09-03 13:14:38 WARNING  # tokens (including EOS '.'): 18682\n",
            "2020-09-03 13:14:38 WARNING  Token-level (P)PPL: 117823.10046313112\n",
            "2020-09-03 13:14:38 WARNING  Word-normalized (P)PPL: 2492563.452886903\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 13:14:41.797568: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 13:14:49 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 13:14:49 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 13:14:49 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 13:14:49 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 13:14:54 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
            "2020-09-03 13:14:54 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 13:14:54 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 13:14:54 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 13:14:54 INFO     loading file None\n",
            "2020-09-03 13:14:54 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 13:14:54 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 13:14:54 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 13:14:58 WARNING  # sentences: 100\n",
            "2020-09-03 13:14:58 INFO     FixedBucketSampler:\n",
            "  sample_num=14676, batch_num=32\n",
            "  key=[146, 147, 148, 149, 150, 151]\n",
            "  cnt=[144, 1595, 3942, 5439, 2960, 596]\n",
            "  batch_size=[517, 513, 510, 506, 503, 500]\n",
            "[13:14:58] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 13:16:42 INFO     9505 sents of 14676, batch 20 of 32\n",
            "2020-09-03 13:17:39 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 13:17:39 WARNING  # words (excluding EOS '.'): 11541\n",
            "2020-09-03 13:17:39 WARNING  longest sentence: 116\n",
            "2020-09-03 13:17:39 WARNING  # tokens (including EOS '.'): 14676\n",
            "2020-09-03 13:17:39 WARNING  Token-level (P)PPL: 72604.2156223721\n",
            "2020-09-03 13:17:39 WARNING  Word-normalized (P)PPL: 1518429.6447582692\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 13:17:42.940799: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 13:17:50 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 13:17:50 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 13:17:50 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 13:17:50 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 13:17:55 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "2020-09-03 13:17:55 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 13:17:55 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 13:17:55 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 13:17:55 INFO     loading file None\n",
            "2020-09-03 13:17:55 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 13:17:55 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 13:17:55 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 13:17:59 WARNING  # sentences: 100\n",
            "2020-09-03 13:17:59 INFO     FixedBucketSampler:\n",
            "  sample_num=23726, batch_num=50\n",
            "  key=[237, 238, 239, 240, 241, 242]\n",
            "  cnt=[1410, 5900, 6636, 5236, 3824, 720]\n",
            "  batch_size=[510, 508, 506, 504, 502, 500]\n",
            "[13:17:59] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 13:20:58 INFO     9584 sents of 23726, batch 20 of 50\n",
            "2020-09-03 13:23:55 INFO     18956 sents of 23726, batch 40 of 50\n",
            "2020-09-03 13:25:26 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 13:25:26 WARNING  # words (excluding EOS '.'): 18204\n",
            "2020-09-03 13:25:26 WARNING  longest sentence: 184\n",
            "2020-09-03 13:25:26 WARNING  # tokens (including EOS '.'): 23726\n",
            "2020-09-03 13:25:26 WARNING  Token-level (P)PPL: 78618.92355786664\n",
            "2020-09-03 13:25:26 WARNING  Word-normalized (P)PPL: 2401794.886844823\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 13:25:30.158526: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 13:25:37 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 13:25:37 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 13:25:37 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 13:25:37 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 13:25:42 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias']\n",
            "2020-09-03 13:25:42 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 13:25:42 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 13:25:42 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 13:25:42 INFO     loading file None\n",
            "2020-09-03 13:25:42 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 13:25:42 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 13:25:42 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 13:25:46 WARNING  # sentences: 100\n",
            "2020-09-03 13:25:47 INFO     FixedBucketSampler:\n",
            "  sample_num=11907, batch_num=26\n",
            "  key=[119, 120, 121, 122, 123]\n",
            "  cnt=[351, 2596, 4998, 3720, 242]\n",
            "  batch_size=[516, 512, 508, 504, 500]\n",
            "[13:25:47] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 13:27:09 INFO     9472 sents of 11907, batch 20 of 26\n",
            "2020-09-03 13:27:30 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 13:27:30 WARNING  # words (excluding EOS '.'): 9998\n",
            "2020-09-03 13:27:30 WARNING  longest sentence: 101\n",
            "2020-09-03 13:27:30 WARNING  # tokens (including EOS '.'): 11907\n",
            "2020-09-03 13:27:30 WARNING  Token-level (P)PPL: 73566.7015641486\n",
            "2020-09-03 13:27:30 WARNING  Word-normalized (P)PPL: 625052.6326563477\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 13:27:33.366402: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 13:27:40 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 13:27:40 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 13:27:40 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 13:27:40 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 13:27:45 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight']\n",
            "2020-09-03 13:27:45 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 13:27:45 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 13:27:45 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 13:27:45 INFO     loading file None\n",
            "2020-09-03 13:27:45 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 13:27:45 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 13:27:45 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 13:27:50 WARNING  # sentences: 100\n",
            "2020-09-03 13:27:50 INFO     FixedBucketSampler:\n",
            "  sample_num=23693, batch_num=49\n",
            "  key=[238, 239, 240]\n",
            "  cnt=[6372, 12561, 4760]\n",
            "  batch_size=[504, 502, 500]\n",
            "[13:27:50] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 13:30:51 INFO     9780 sents of 23693, batch 20 of 49\n",
            "2020-09-03 13:33:53 INFO     19337 sents of 23693, batch 40 of 49\n",
            "2020-09-03 13:35:16 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 13:35:16 WARNING  # words (excluding EOS '.'): 18393\n",
            "2020-09-03 13:35:16 WARNING  longest sentence: 184\n",
            "2020-09-03 13:35:16 WARNING  # tokens (including EOS '.'): 23693\n",
            "2020-09-03 13:35:16 WARNING  Token-level (P)PPL: 87160.06645122138\n",
            "2020-09-03 13:35:16 WARNING  Word-normalized (P)PPL: 2311454.659834272\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 13:35:20.065439: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 13:35:27 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 13:35:27 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 13:35:27 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 13:35:27 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 13:35:32 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
            "2020-09-03 13:35:32 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 13:35:32 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 13:35:32 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 13:35:32 INFO     loading file None\n",
            "2020-09-03 13:35:32 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 13:35:32 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 13:35:32 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 13:35:37 WARNING  # sentences: 100\n",
            "2020-09-03 13:35:37 INFO     FixedBucketSampler:\n",
            "  sample_num=13322, batch_num=30\n",
            "  key=[132, 133, 134, 135, 136, 137]\n",
            "  cnt=[130, 1179, 2112, 3724, 4422, 1755]\n",
            "  batch_size=[518, 515, 511, 507, 503, 500]\n",
            "[13:35:37] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 13:37:13 INFO     9726 sents of 13322, batch 20 of 30\n",
            "2020-09-03 13:37:49 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 13:37:49 WARNING  # words (excluding EOS '.'): 10854\n",
            "2020-09-03 13:37:49 WARNING  longest sentence: 109\n",
            "2020-09-03 13:37:49 WARNING  # tokens (including EOS '.'): 13322\n",
            "2020-09-03 13:37:49 WARNING  Token-level (P)PPL: 73365.12427737124\n",
            "2020-09-03 13:37:49 WARNING  Word-normalized (P)PPL: 937157.3844135265\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 13:37:52.441870: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 13:37:59 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 13:37:59 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 13:37:59 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 13:37:59 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 13:38:05 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias']\n",
            "2020-09-03 13:38:05 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 13:38:05 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 13:38:05 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 13:38:05 INFO     loading file None\n",
            "2020-09-03 13:38:05 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 13:38:05 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 13:38:05 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 13:38:09 WARNING  # sentences: 100\n",
            "2020-09-03 13:38:09 INFO     FixedBucketSampler:\n",
            "  sample_num=17969, batch_num=40\n",
            "  key=[178, 179, 180, 181, 182, 183]\n",
            "  cnt=[176, 1593, 2670, 1074, 5940, 6516]\n",
            "  batch_size=[514, 511, 508, 505, 502, 500]\n",
            "[13:38:09] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 13:40:20 INFO     9528 sents of 17969, batch 20 of 40\n",
            "2020-09-03 13:42:17 INFO     17969 sents of 17969, batch 40 of 40\n",
            "2020-09-03 13:42:17 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 13:42:17 WARNING  # words (excluding EOS '.'): 13453\n",
            "2020-09-03 13:42:17 WARNING  longest sentence: 135\n",
            "2020-09-03 13:42:17 WARNING  # tokens (including EOS '.'): 17969\n",
            "2020-09-03 13:42:17 WARNING  Token-level (P)PPL: 90320.30599555536\n",
            "2020-09-03 13:42:17 WARNING  Word-normalized (P)PPL: 4162738.8031240785\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 13:42:20.485424: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 13:42:27 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 13:42:27 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 13:42:27 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 13:42:27 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 13:42:33 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
            "2020-09-03 13:42:33 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 13:42:33 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 13:42:33 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 13:42:33 INFO     loading file None\n",
            "2020-09-03 13:42:33 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 13:42:33 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 13:42:33 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 13:42:37 WARNING  # sentences: 100\n",
            "2020-09-03 13:42:37 INFO     FixedBucketSampler:\n",
            "  sample_num=11598, batch_num=26\n",
            "  key=[116, 117, 118, 119, 120, 121, 122, 123]\n",
            "  cnt=[798, 4255, 4292, 936, 236, 357, 240, 484]\n",
            "  batch_size=[530, 525, 521, 516, 512, 508, 504, 500]\n",
            "[13:42:37] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 13:43:54 INFO     9170 sents of 11598, batch 20 of 26\n",
            "2020-09-03 13:44:14 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 13:44:14 WARNING  # words (excluding EOS '.'): 8898\n",
            "2020-09-03 13:44:14 WARNING  longest sentence: 89\n",
            "2020-09-03 13:44:14 WARNING  # tokens (including EOS '.'): 11598\n",
            "2020-09-03 13:44:14 WARNING  Token-level (P)PPL: 113883.17454956738\n",
            "2020-09-03 13:44:14 WARNING  Word-normalized (P)PPL: 3897503.1292390157\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 13:44:17.761255: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 13:44:25 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 13:44:25 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 13:44:25 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 13:44:25 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 13:44:30 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
            "2020-09-03 13:44:30 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 13:44:30 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 13:44:30 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 13:44:30 INFO     loading file None\n",
            "2020-09-03 13:44:30 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 13:44:30 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 13:44:30 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 13:44:34 WARNING  # sentences: 100\n",
            "2020-09-03 13:44:35 INFO     FixedBucketSampler:\n",
            "  sample_num=20758, batch_num=45\n",
            "  key=[207, 208, 209, 210, 211, 212]\n",
            "  cnt=[205, 2678, 6624, 7488, 3553, 210]\n",
            "  batch_size=[512, 509, 507, 504, 502, 500]\n",
            "[13:44:35] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 13:47:04 INFO     9307 sents of 20758, batch 20 of 45\n",
            "2020-09-03 13:49:41 INFO     18893 sents of 20758, batch 40 of 45\n",
            "2020-09-03 13:50:11 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 13:50:11 WARNING  # words (excluding EOS '.'): 17341\n",
            "2020-09-03 13:50:11 WARNING  longest sentence: 174\n",
            "2020-09-03 13:50:11 WARNING  # tokens (including EOS '.'): 20758\n",
            "2020-09-03 13:50:11 WARNING  Token-level (P)PPL: 111823.83334754112\n",
            "2020-09-03 13:50:11 WARNING  Word-normalized (P)PPL: 1104931.233891797\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 13:50:14.853133: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 13:50:22 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 13:50:22 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 13:50:22 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 13:50:22 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 13:50:27 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "2020-09-03 13:50:27 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 13:50:27 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 13:50:27 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 13:50:27 INFO     loading file None\n",
            "2020-09-03 13:50:27 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 13:50:27 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 13:50:27 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 13:50:31 WARNING  # sentences: 100\n",
            "2020-09-03 13:50:32 INFO     FixedBucketSampler:\n",
            "  sample_num=15598, batch_num=34\n",
            "  key=[154, 155, 156, 157, 158, 159, 160, 161]\n",
            "  cnt=[152, 459, 1540, 3255, 4524, 3297, 2212, 159]\n",
            "  batch_size=[522, 519, 516, 512, 509, 506, 503, 500]\n",
            "[13:50:32] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 13:52:20 INFO     9231 sents of 15598, batch 20 of 34\n",
            "2020-09-03 13:53:35 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 13:53:35 WARNING  # words (excluding EOS '.'): 12498\n",
            "2020-09-03 13:53:35 WARNING  longest sentence: 126\n",
            "2020-09-03 13:53:35 WARNING  # tokens (including EOS '.'): 15598\n",
            "2020-09-03 13:53:35 WARNING  Token-level (P)PPL: 100504.87429406772\n",
            "2020-09-03 13:53:35 WARNING  Word-normalized (P)PPL: 1749556.8145838112\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 13:53:39.278708: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 13:53:46 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 13:53:46 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 13:53:46 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 13:53:46 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 13:53:51 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
            "2020-09-03 13:53:51 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 13:53:51 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 13:53:51 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 13:53:51 INFO     loading file None\n",
            "2020-09-03 13:53:51 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 13:53:51 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 13:53:51 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 13:53:56 WARNING  # sentences: 100\n",
            "2020-09-03 13:53:56 INFO     FixedBucketSampler:\n",
            "  sample_num=31163, batch_num=64\n",
            "  key=[312, 313, 314, 315]\n",
            "  cnt=[2480, 11818, 11544, 5321]\n",
            "  batch_size=[504, 503, 501, 500]\n",
            "[13:53:56] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 13:58:12 INFO     9830 sents of 31163, batch 20 of 64\n",
            "2020-09-03 14:02:26 INFO     19380 sents of 31163, batch 40 of 64\n",
            "2020-09-03 14:06:47 INFO     29187 sents of 31163, batch 60 of 64\n",
            "2020-09-03 14:07:39 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 14:07:39 WARNING  # words (excluding EOS '.'): 23790\n",
            "2020-09-03 14:07:39 WARNING  longest sentence: 238\n",
            "2020-09-03 14:07:39 WARNING  # tokens (including EOS '.'): 31163\n",
            "2020-09-03 14:07:39 WARNING  Token-level (P)PPL: 89691.30701364267\n",
            "2020-09-03 14:07:39 WARNING  Word-normalized (P)PPL: 3074025.0929297213\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 14:07:42.874061: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 14:07:50 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 14:07:50 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 14:07:50 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 14:07:50 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 14:07:55 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
            "2020-09-03 14:07:55 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 14:07:55 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 14:07:55 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 14:07:55 INFO     loading file None\n",
            "2020-09-03 14:07:55 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 14:07:55 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 14:07:55 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 14:07:59 WARNING  # sentences: 100\n",
            "2020-09-03 14:08:00 INFO     FixedBucketSampler:\n",
            "  sample_num=53249, batch_num=109\n",
            "  key=[532, 533, 534, 535, 536]\n",
            "  cnt=[1060, 5841, 15960, 26650, 3738]\n",
            "  batch_size=[503, 502, 501, 500, 500]\n",
            "[14:08:00] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [64,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [65,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [66,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [67,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [68,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [69,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [70,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [71,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [72,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [73,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [74,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [75,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [76,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [77,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [78,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [79,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [80,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [81,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [82,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [83,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [84,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [85,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [86,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [87,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [88,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [89,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [90,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [91,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [92,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [93,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [94,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "/pytorch/aten/src/THC/THCTensorIndex.cu:272: indexSelectLargeIndex: block: [214,0,0], thread: [95,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/mlm\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/mlm/cmds.py\", line 184, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/mlm/cmds.py\", line 234, in cmd_score\n",
            "    scores, true_tok_lens = scorer.score(corpus_for_scoring, ratio=1, split_size=args.split_size, per_token=args.per_token)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/mlm/scorers.py\", line 677, in score\n",
            "    out = self._model(input_ids=token_ids, attention_mask=mask, select_positions=masked_positions)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/data_parallel.py\", line 153, in forward\n",
            "    return self.module(*inputs[0], **kwargs[0])\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/mlm/models/bert.py\", line 203, in forward\n",
            "    encoder_attention_mask=encoder_attention_mask,\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\", line 736, in forward\n",
            "    encoder_attention_mask=encoder_extended_attention_mask,\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\", line 407, in forward\n",
            "    hidden_states, attention_mask, head_mask[i], encoder_hidden_states, encoder_attention_mask\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\", line 368, in forward\n",
            "    self_attention_outputs = self.attention(hidden_states, attention_mask, head_mask)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\", line 314, in forward\n",
            "    hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/transformers/modeling_bert.py\", line 216, in forward\n",
            "    mixed_query_layer = self.query(hidden_states)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\", line 722, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\", line 91, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\", line 1676, in linear\n",
            "    output = input.matmul(weight.t())\n",
            "RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 14:08:04.692465: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 14:08:11 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 14:08:11 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 14:08:11 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 14:08:11 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 14:08:16 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "2020-09-03 14:08:16 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 14:08:16 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 14:08:16 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 14:08:16 INFO     loading file None\n",
            "2020-09-03 14:08:16 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 14:08:16 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 14:08:16 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 14:08:21 WARNING  # sentences: 100\n",
            "2020-09-03 14:08:22 INFO     FixedBucketSampler:\n",
            "  sample_num=40855, batch_num=85\n",
            "  key=[409, 410, 411, 412, 413]\n",
            "  cnt=[10175, 12648, 6135, 9020, 2877]\n",
            "  batch_size=[504, 503, 502, 501, 500]\n",
            "[14:08:22] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 14:14:10 INFO     9891 sents of 40855, batch 20 of 85\n",
            "2020-09-03 14:19:48 INFO     19038 sents of 40855, batch 40 of 85\n",
            "2020-09-03 14:26:00 INFO     29098 sents of 40855, batch 60 of 85\n",
            "2020-09-03 14:31:56 INFO     38744 sents of 40855, batch 80 of 85\n",
            "2020-09-03 14:33:14 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 14:33:14 WARNING  # words (excluding EOS '.'): 31642\n",
            "2020-09-03 14:33:14 WARNING  longest sentence: 316\n",
            "2020-09-03 14:33:14 WARNING  # tokens (including EOS '.'): 40855\n",
            "2020-09-03 14:33:14 WARNING  Token-level (P)PPL: 74924.39156853368\n",
            "2020-09-03 14:33:14 WARNING  Word-normalized (P)PPL: 1967597.5928492143\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 14:33:17.393006: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 14:33:24 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 14:33:24 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 14:33:24 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 14:33:24 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 14:33:29 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
            "2020-09-03 14:33:29 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 14:33:29 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 14:33:29 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 14:33:29 INFO     loading file None\n",
            "2020-09-03 14:33:29 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 14:33:29 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 14:33:29 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 14:33:33 WARNING  # sentences: 100\n",
            "2020-09-03 14:33:34 INFO     FixedBucketSampler:\n",
            "  sample_num=33641, batch_num=70\n",
            "  key=[336, 337, 338, 339, 340, 341]\n",
            "  cnt=[1336, 2680, 13776, 12806, 2704, 339]\n",
            "  batch_size=[507, 505, 504, 502, 501, 500]\n",
            "[14:33:34] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 14:38:19 INFO     9569 sents of 33641, batch 20 of 70\n",
            "2020-09-03 14:43:11 INFO     19377 sents of 33641, batch 40 of 70\n",
            "2020-09-03 14:48:11 INFO     29457 sents of 33641, batch 60 of 70\n",
            "2020-09-03 14:50:14 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 14:50:14 WARNING  # words (excluding EOS '.'): 26447\n",
            "2020-09-03 14:50:14 WARNING  longest sentence: 266\n",
            "2020-09-03 14:50:14 WARNING  # tokens (including EOS '.'): 33641\n",
            "2020-09-03 14:50:14 WARNING  Token-level (P)PPL: 84455.7529713276\n",
            "2020-09-03 14:50:14 WARNING  Word-normalized (P)PPL: 1848203.9106352548\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 14:50:17.936017: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 14:50:25 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 14:50:25 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 14:50:25 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 14:50:25 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 14:50:29 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "2020-09-03 14:50:29 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 14:50:29 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 14:50:29 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 14:50:29 INFO     loading file None\n",
            "2020-09-03 14:50:29 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 14:50:29 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 14:50:29 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 14:50:33 WARNING  # sentences: 100\n",
            "2020-09-03 14:50:34 INFO     FixedBucketSampler:\n",
            "  sample_num=18841, batch_num=41\n",
            "  key=[188, 189, 190, 191, 192]\n",
            "  cnt=[558, 2057, 6956, 7560, 1710]\n",
            "  batch_size=[510, 507, 505, 502, 500]\n",
            "[14:50:34] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 14:52:56 INFO     9270 sents of 18841, batch 20 of 41\n",
            "2020-09-03 14:55:20 INFO     18793 sents of 18841, batch 40 of 41\n",
            "2020-09-03 14:55:21 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 14:55:21 WARNING  # words (excluding EOS '.'): 13766\n",
            "2020-09-03 14:55:21 WARNING  longest sentence: 139\n",
            "2020-09-03 14:55:21 WARNING  # tokens (including EOS '.'): 18841\n",
            "2020-09-03 14:55:21 WARNING  Token-level (P)PPL: 144504.93871731195\n",
            "2020-09-03 14:55:21 WARNING  Word-normalized (P)PPL: 11538117.388536995\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 14:55:24.177416: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 14:55:31 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 14:55:31 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 14:55:31 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 14:55:31 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 14:55:36 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "2020-09-03 14:55:36 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 14:55:36 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 14:55:36 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 14:55:36 INFO     loading file None\n",
            "2020-09-03 14:55:36 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 14:55:36 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 14:55:36 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 14:55:40 WARNING  # sentences: 100\n",
            "2020-09-03 14:55:40 INFO     FixedBucketSampler:\n",
            "  sample_num=16708, batch_num=36\n",
            "  key=[167, 168, 169, 170]\n",
            "  cnt=[165, 3320, 8183, 5040]\n",
            "  batch_size=[508, 505, 502, 500]\n",
            "[14:55:40] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 14:57:50 INFO     9558 sents of 16708, batch 20 of 36\n",
            "2020-09-03 14:59:26 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 14:59:26 WARNING  # words (excluding EOS '.'): 12350\n",
            "2020-09-03 14:59:26 WARNING  longest sentence: 123\n",
            "2020-09-03 14:59:26 WARNING  # tokens (including EOS '.'): 16708\n",
            "2020-09-03 14:59:26 WARNING  Token-level (P)PPL: 90487.2201587683\n",
            "2020-09-03 14:59:26 WARNING  Word-normalized (P)PPL: 5077389.291920859\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 14:59:29.499771: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 14:59:36 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 14:59:36 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 14:59:36 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 14:59:36 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 14:59:41 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias']\n",
            "2020-09-03 14:59:41 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 14:59:41 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 14:59:41 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 14:59:41 INFO     loading file None\n",
            "2020-09-03 14:59:41 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 14:59:41 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 14:59:41 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 14:59:45 WARNING  # sentences: 100\n",
            "2020-09-03 14:59:46 INFO     FixedBucketSampler:\n",
            "  sample_num=15502, batch_num=33\n",
            "  key=[155, 156, 157, 158, 159, 160]\n",
            "  cnt=[918, 3696, 6510, 3120, 942, 316]\n",
            "  batch_size=[516, 512, 509, 506, 503, 500]\n",
            "[14:59:46] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 15:01:45 INFO     9468 sents of 15502, batch 20 of 33\n",
            "2020-09-03 15:02:59 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 15:02:59 WARNING  # words (excluding EOS '.'): 12100\n",
            "2020-09-03 15:02:59 WARNING  longest sentence: 122\n",
            "2020-09-03 15:02:59 WARNING  # tokens (including EOS '.'): 15502\n",
            "2020-09-03 15:02:59 WARNING  Token-level (P)PPL: 71775.27620052865\n",
            "2020-09-03 15:02:59 WARNING  Word-normalized (P)PPL: 1664433.0084681343\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 15:03:02.583653: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 15:03:09 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 15:03:09 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 15:03:09 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 15:03:09 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 15:03:14 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
            "2020-09-03 15:03:14 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 15:03:14 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 15:03:14 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 15:03:14 INFO     loading file None\n",
            "2020-09-03 15:03:14 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 15:03:14 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 15:03:14 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 15:03:18 WARNING  # sentences: 100\n",
            "2020-09-03 15:03:19 INFO     FixedBucketSampler:\n",
            "  sample_num=24701, batch_num=51\n",
            "  key=[247, 248, 249, 250, 251]\n",
            "  cnt=[735, 6396, 10374, 6200, 996]\n",
            "  batch_size=[508, 506, 504, 502, 500]\n",
            "[15:03:19] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 15:06:39 INFO     9716 sents of 24701, batch 20 of 51\n",
            "2020-09-03 15:10:02 INFO     19594 sents of 24701, batch 40 of 51\n",
            "2020-09-03 15:11:45 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 15:11:45 WARNING  # words (excluding EOS '.'): 19213\n",
            "2020-09-03 15:11:45 WARNING  longest sentence: 193\n",
            "2020-09-03 15:11:45 WARNING  # tokens (including EOS '.'): 24701\n",
            "2020-09-03 15:11:45 WARNING  Token-level (P)PPL: 61330.24727457766\n",
            "2020-09-03 15:11:45 WARNING  Word-normalized (P)PPL: 1429637.0430571765\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 15:11:48.578570: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 15:11:55 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 15:11:55 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 15:11:55 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 15:11:55 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 15:12:00 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "2020-09-03 15:12:00 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 15:12:00 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 15:12:00 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 15:12:00 INFO     loading file None\n",
            "2020-09-03 15:12:00 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 15:12:00 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 15:12:00 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 15:12:04 WARNING  # sentences: 100\n",
            "2020-09-03 15:12:04 INFO     FixedBucketSampler:\n",
            "  sample_num=21291, batch_num=44\n",
            "  key=[213, 214, 215, 216]\n",
            "  cnt=[1477, 4240, 10224, 5350]\n",
            "  batch_size=[507, 504, 502, 500]\n",
            "[15:12:04] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 15:14:58 INFO     9868 sents of 21291, batch 20 of 44\n",
            "2020-09-03 15:17:48 INFO     19606 sents of 21291, batch 40 of 44\n",
            "2020-09-03 15:18:18 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 15:18:18 WARNING  # words (excluding EOS '.'): 18678\n",
            "2020-09-03 15:18:18 WARNING  longest sentence: 187\n",
            "2020-09-03 15:18:18 WARNING  # tokens (including EOS '.'): 21291\n",
            "2020-09-03 15:18:18 WARNING  Token-level (P)PPL: 96774.52886738071\n",
            "2020-09-03 15:18:18 WARNING  Word-normalized (P)PPL: 482230.977305668\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 15:18:21.222752: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 15:18:28 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 15:18:28 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 15:18:28 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 15:18:28 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 15:18:33 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "2020-09-03 15:18:33 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 15:18:33 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 15:18:33 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 15:18:33 INFO     loading file None\n",
            "2020-09-03 15:18:33 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 15:18:33 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 15:18:33 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 15:18:37 WARNING  # sentences: 100\n",
            "2020-09-03 15:18:37 INFO     FixedBucketSampler:\n",
            "  sample_num=15852, batch_num=35\n",
            "  key=[158, 159, 160, 161, 162, 163, 164, 165]\n",
            "  cnt=[1404, 3925, 4582, 1908, 640, 1610, 1620, 163]\n",
            "  batch_size=[522, 518, 515, 512, 509, 506, 503, 500]\n",
            "[15:18:37] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 15:20:26 INFO     8516 sents of 15852, batch 20 of 35\n",
            "2020-09-03 15:21:59 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 15:21:59 WARNING  # words (excluding EOS '.'): 12694\n",
            "2020-09-03 15:21:59 WARNING  longest sentence: 128\n",
            "2020-09-03 15:21:59 WARNING  # tokens (including EOS '.'): 15852\n",
            "2020-09-03 15:21:59 WARNING  Token-level (P)PPL: 65831.6992931003\n",
            "2020-09-03 15:21:59 WARNING  Word-normalized (P)PPL: 1040304.0400750613\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 15:22:02.430073: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 15:22:09 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 15:22:09 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 15:22:09 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 15:22:09 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 15:22:14 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
            "2020-09-03 15:22:14 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 15:22:14 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 15:22:14 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 15:22:14 INFO     loading file None\n",
            "2020-09-03 15:22:14 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 15:22:14 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 15:22:14 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 15:22:18 WARNING  # sentences: 100\n",
            "2020-09-03 15:22:18 INFO     FixedBucketSampler:\n",
            "  sample_num=15867, batch_num=33\n",
            "  key=[159, 160, 161, 162, 163]\n",
            "  cnt=[942, 6004, 6519, 2080, 322]\n",
            "  batch_size=[512, 509, 506, 503, 500]\n",
            "[15:22:18] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 15:24:20 INFO     9430 sents of 15867, batch 20 of 33\n",
            "2020-09-03 15:25:41 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 15:25:41 WARNING  # words (excluding EOS '.'): 12134\n",
            "2020-09-03 15:25:41 WARNING  longest sentence: 121\n",
            "2020-09-03 15:25:41 WARNING  # tokens (including EOS '.'): 15867\n",
            "2020-09-03 15:25:41 WARNING  Token-level (P)PPL: 58120.460733363645\n",
            "2020-09-03 15:25:41 WARNING  Word-normalized (P)PPL: 1698497.9411953324\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 15:25:44.418065: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 15:25:51 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 15:25:51 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 15:25:51 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 15:25:51 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 15:25:56 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
            "2020-09-03 15:25:56 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 15:25:56 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 15:25:56 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 15:25:56 INFO     loading file None\n",
            "2020-09-03 15:25:56 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 15:25:56 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 15:25:56 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 15:26:00 WARNING  # sentences: 100\n",
            "2020-09-03 15:26:00 INFO     FixedBucketSampler:\n",
            "  sample_num=12119, batch_num=26\n",
            "  key=[121, 122, 123, 124, 125]\n",
            "  cnt=[1190, 2160, 3267, 4026, 1476]\n",
            "  batch_size=[516, 512, 508, 504, 500]\n",
            "[15:26:00] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 15:27:33 INFO     9793 sents of 12119, batch 20 of 26\n",
            "2020-09-03 15:27:54 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 15:27:54 WARNING  # words (excluding EOS '.'): 9951\n",
            "2020-09-03 15:27:54 WARNING  longest sentence: 100\n",
            "2020-09-03 15:27:54 WARNING  # tokens (including EOS '.'): 12119\n",
            "2020-09-03 15:27:54 WARNING  Token-level (P)PPL: 70136.77738292638\n",
            "2020-09-03 15:27:54 WARNING  Word-normalized (P)PPL: 797480.6469391568\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 15:27:57.529710: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 15:28:04 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 15:28:04 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 15:28:04 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 15:28:04 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 15:28:09 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "2020-09-03 15:28:09 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 15:28:09 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 15:28:09 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 15:28:09 INFO     loading file None\n",
            "2020-09-03 15:28:09 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 15:28:09 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 15:28:09 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 15:28:13 WARNING  # sentences: 100\n",
            "2020-09-03 15:28:13 INFO     FixedBucketSampler:\n",
            "  sample_num=23667, batch_num=49\n",
            "  key=[237, 238, 239, 240]\n",
            "  cnt=[2350, 7316, 9717, 4284]\n",
            "  batch_size=[506, 504, 502, 500]\n",
            "[15:28:13] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 15:31:26 INFO     9806 sents of 23667, batch 20 of 49\n",
            "2020-09-03 15:34:37 INFO     19545 sents of 23667, batch 40 of 49\n",
            "2020-09-03 15:35:57 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 15:35:57 WARNING  # words (excluding EOS '.'): 17741\n",
            "2020-09-03 15:35:57 WARNING  longest sentence: 177\n",
            "2020-09-03 15:35:57 WARNING  # tokens (including EOS '.'): 23667\n",
            "2020-09-03 15:35:57 WARNING  Token-level (P)PPL: 85596.04283354469\n",
            "2020-09-03 15:35:57 WARNING  Word-normalized (P)PPL: 3802190.741488993\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 15:36:00.835848: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 15:36:07 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 15:36:07 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 15:36:07 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 15:36:07 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 15:36:12 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "2020-09-03 15:36:12 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 15:36:12 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 15:36:12 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 15:36:12 INFO     loading file None\n",
            "2020-09-03 15:36:12 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 15:36:12 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 15:36:12 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 15:36:16 WARNING  # sentences: 100\n",
            "2020-09-03 15:36:16 INFO     FixedBucketSampler:\n",
            "  sample_num=19965, batch_num=42\n",
            "  key=[200, 201, 202, 203, 204]\n",
            "  cnt=[2178, 6766, 7200, 3417, 404]\n",
            "  batch_size=[510, 507, 504, 502, 500]\n",
            "[15:36:16] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 15:39:00 INFO     9869 sents of 19965, batch 20 of 42\n",
            "2020-09-03 15:41:33 INFO     19317 sents of 19965, batch 40 of 42\n",
            "2020-09-03 15:41:44 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 15:41:44 WARNING  # words (excluding EOS '.'): 15916\n",
            "2020-09-03 15:41:44 WARNING  longest sentence: 159\n",
            "2020-09-03 15:41:44 WARNING  # tokens (including EOS '.'): 19965\n",
            "2020-09-03 15:41:44 WARNING  Token-level (P)PPL: 85461.44477232205\n",
            "2020-09-03 15:41:44 WARNING  Word-normalized (P)PPL: 1536042.235499805\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 15:41:47.110851: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 15:41:54 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 15:41:54 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 15:41:54 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 15:41:54 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 15:41:59 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "2020-09-03 15:41:59 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 15:41:59 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 15:41:59 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 15:41:59 INFO     loading file None\n",
            "2020-09-03 15:41:59 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 15:41:59 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 15:41:59 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 15:42:03 WARNING  # sentences: 100\n",
            "2020-09-03 15:42:03 INFO     FixedBucketSampler:\n",
            "  sample_num=15544, batch_num=33\n",
            "  key=[156, 157, 158, 159]\n",
            "  cnt=[1232, 6665, 7176, 471]\n",
            "  batch_size=[509, 506, 503, 500]\n",
            "[15:42:03] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 15:44:05 INFO     9671 sents of 15544, batch 20 of 33\n",
            "2020-09-03 15:45:17 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 15:45:17 WARNING  # words (excluding EOS '.'): 12540\n",
            "2020-09-03 15:45:17 WARNING  longest sentence: 125\n",
            "2020-09-03 15:45:17 WARNING  # tokens (including EOS '.'): 15544\n",
            "2020-09-03 15:45:17 WARNING  Token-level (P)PPL: 104842.07099503964\n",
            "2020-09-03 15:45:17 WARNING  Word-normalized (P)PPL: 1671945.4648209037\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 15:45:21.070193: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 15:45:28 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 15:45:28 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 15:45:28 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 15:45:28 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 15:45:32 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
            "2020-09-03 15:45:32 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 15:45:32 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 15:45:32 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 15:45:32 INFO     loading file None\n",
            "2020-09-03 15:45:32 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 15:45:32 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 15:45:32 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 15:45:36 WARNING  # sentences: 100\n",
            "2020-09-03 15:45:37 INFO     FixedBucketSampler:\n",
            "  sample_num=16228, batch_num=36\n",
            "  key=[161, 162, 163, 164, 165, 166, 167]\n",
            "  cnt=[1113, 2560, 1127, 2268, 4727, 3608, 825]\n",
            "  batch_size=[518, 515, 512, 509, 506, 503, 500]\n",
            "[15:45:37] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 15:47:38 INFO     9160 sents of 16228, batch 20 of 36\n",
            "2020-09-03 15:49:09 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 15:49:09 WARNING  # words (excluding EOS '.'): 13302\n",
            "2020-09-03 15:49:09 WARNING  longest sentence: 133\n",
            "2020-09-03 15:49:09 WARNING  # tokens (including EOS '.'): 16228\n",
            "2020-09-03 15:49:09 WARNING  Token-level (P)PPL: 76544.27355711558\n",
            "2020-09-03 15:49:09 WARNING  Word-normalized (P)PPL: 908263.6892916034\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 15:49:12.577516: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 15:49:19 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 15:49:19 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 15:49:19 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 15:49:19 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 15:49:24 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias']\n",
            "2020-09-03 15:49:24 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 15:49:24 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 15:49:24 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 15:49:24 INFO     loading file None\n",
            "2020-09-03 15:49:24 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 15:49:24 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 15:49:24 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 15:49:28 WARNING  # sentences: 100\n",
            "2020-09-03 15:49:28 INFO     FixedBucketSampler:\n",
            "  sample_num=19309, batch_num=41\n",
            "  key=[192, 193, 194, 195, 196, 197]\n",
            "  cnt=[190, 955, 4224, 6948, 5432, 1560]\n",
            "  batch_size=[513, 510, 507, 505, 502, 500]\n",
            "[15:49:28] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 15:52:00 INFO     9517 sents of 19309, batch 20 of 41\n",
            "2020-09-03 15:54:31 INFO     19119 sents of 19309, batch 40 of 41\n",
            "2020-09-03 15:54:34 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 15:54:34 WARNING  # words (excluding EOS '.'): 14368\n",
            "2020-09-03 15:54:34 WARNING  longest sentence: 145\n",
            "2020-09-03 15:54:34 WARNING  # tokens (including EOS '.'): 19309\n",
            "2020-09-03 15:54:34 WARNING  Token-level (P)PPL: 48002.58874494211\n",
            "2020-09-03 15:54:34 WARNING  Word-normalized (P)PPL: 1954794.0546804543\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 15:54:37.200375: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 15:54:44 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 15:54:44 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 15:54:44 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 15:54:44 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 15:54:48 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
            "2020-09-03 15:54:48 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 15:54:48 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 15:54:48 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 15:54:48 INFO     loading file None\n",
            "2020-09-03 15:54:48 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 15:54:48 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 15:54:48 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 15:54:52 WARNING  # sentences: 100\n",
            "2020-09-03 15:54:53 INFO     FixedBucketSampler:\n",
            "  sample_num=10822, batch_num=24\n",
            "  key=[108, 109, 110, 111, 112, 113, 114]\n",
            "  cnt=[848, 2461, 3456, 2398, 880, 555, 224]\n",
            "  batch_size=[527, 522, 518, 513, 508, 504, 500]\n",
            "[15:54:53] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 15:56:09 INFO     9079 sents of 10822, batch 20 of 24\n",
            "2020-09-03 15:56:23 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 15:56:23 WARNING  # words (excluding EOS '.'): 8791\n",
            "2020-09-03 15:56:23 WARNING  longest sentence: 89\n",
            "2020-09-03 15:56:23 WARNING  # tokens (including EOS '.'): 10822\n",
            "2020-09-03 15:56:23 WARNING  Token-level (P)PPL: 71125.64018764018\n",
            "2020-09-03 15:56:23 WARNING  Word-normalized (P)PPL: 939719.4735539331\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 15:56:26.544342: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 15:56:33 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 15:56:33 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 15:56:33 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 15:56:33 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 15:56:38 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
            "2020-09-03 15:56:38 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 15:56:38 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 15:56:38 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 15:56:38 INFO     loading file None\n",
            "2020-09-03 15:56:38 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 15:56:38 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 15:56:38 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 15:56:42 WARNING  # sentences: 100\n",
            "2020-09-03 15:56:42 INFO     FixedBucketSampler:\n",
            "  sample_num=16735, batch_num=36\n",
            "  key=[168, 169, 170, 171]\n",
            "  cnt=[3320, 5177, 7224, 1014]\n",
            "  batch_size=[508, 505, 502, 500]\n",
            "[15:56:42] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 15:58:45 INFO     9248 sents of 16735, batch 20 of 36\n",
            "2020-09-03 16:00:26 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 16:00:26 WARNING  # words (excluding EOS '.'): 13196\n",
            "2020-09-03 16:00:26 WARNING  longest sentence: 132\n",
            "2020-09-03 16:00:26 WARNING  # tokens (including EOS '.'): 16735\n",
            "2020-09-03 16:00:26 WARNING  Token-level (P)PPL: 109422.09626354894\n",
            "2020-09-03 16:00:26 WARNING  Word-normalized (P)PPL: 2457701.5973296827\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 16:00:29.143695: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 16:00:36 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 16:00:36 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 16:00:36 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 16:00:36 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 16:00:40 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias']\n",
            "2020-09-03 16:00:40 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 16:00:40 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 16:00:40 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 16:00:40 INFO     loading file None\n",
            "2020-09-03 16:00:40 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 16:00:40 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 16:00:40 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 16:00:44 WARNING  # sentences: 100\n",
            "2020-09-03 16:00:45 INFO     FixedBucketSampler:\n",
            "  sample_num=12635, batch_num=28\n",
            "  key=[126, 127, 128, 129, 130]\n",
            "  cnt=[248, 2375, 4284, 4064, 1664]\n",
            "  batch_size=[515, 511, 507, 503, 500]\n",
            "[16:00:45] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 16:02:15 INFO     9277 sents of 12635, batch 20 of 28\n",
            "2020-09-03 16:02:48 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 16:02:48 WARNING  # words (excluding EOS '.'): 10266\n",
            "2020-09-03 16:02:48 WARNING  longest sentence: 103\n",
            "2020-09-03 16:02:48 WARNING  # tokens (including EOS '.'): 12635\n",
            "2020-09-03 16:02:48 WARNING  Token-level (P)PPL: 83908.63781913754\n",
            "2020-09-03 16:02:48 WARNING  Word-normalized (P)PPL: 1148240.4831553681\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 16:02:51.132363: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 16:02:58 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 16:02:58 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 16:02:58 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 16:02:58 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 16:03:02 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "2020-09-03 16:03:02 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 16:03:02 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 16:03:02 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 16:03:02 INFO     loading file None\n",
            "2020-09-03 16:03:02 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 16:03:02 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 16:03:02 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 16:03:06 WARNING  # sentences: 100\n",
            "2020-09-03 16:03:07 INFO     FixedBucketSampler:\n",
            "  sample_num=16993, batch_num=36\n",
            "  key=[170, 171, 172, 173, 174]\n",
            "  cnt=[840, 4732, 6970, 3591, 860]\n",
            "  batch_size=[511, 508, 505, 502, 500]\n",
            "[16:03:07] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 16:05:15 INFO     9501 sents of 16993, batch 20 of 36\n",
            "2020-09-03 16:06:57 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 16:06:57 WARNING  # words (excluding EOS '.'): 12985\n",
            "2020-09-03 16:06:57 WARNING  longest sentence: 129\n",
            "2020-09-03 16:06:57 WARNING  # tokens (including EOS '.'): 16993\n",
            "2020-09-03 16:06:57 WARNING  Token-level (P)PPL: 54600.919508079714\n",
            "2020-09-03 16:06:57 WARNING  Word-normalized (P)PPL: 1582713.4741169105\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 16:07:00.622435: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 16:07:07 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 16:07:07 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 16:07:07 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 16:07:07 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 16:07:12 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
            "2020-09-03 16:07:12 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 16:07:12 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 16:07:12 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 16:07:12 INFO     loading file None\n",
            "2020-09-03 16:07:12 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 16:07:12 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 16:07:12 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 16:07:16 WARNING  # sentences: 100\n",
            "2020-09-03 16:07:16 INFO     FixedBucketSampler:\n",
            "  sample_num=23340, batch_num=49\n",
            "  key=[233, 234, 235, 236, 237]\n",
            "  cnt=[231, 1624, 10951, 9594, 940]\n",
            "  batch_size=[508, 506, 504, 502, 500]\n",
            "[16:07:16] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 16:10:29 INFO     9976 sents of 23340, batch 20 of 49\n",
            "2020-09-03 16:13:34 INFO     19606 sents of 23340, batch 40 of 49\n",
            "2020-09-03 16:14:45 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 16:14:45 WARNING  # words (excluding EOS '.'): 18501\n",
            "2020-09-03 16:14:45 WARNING  longest sentence: 185\n",
            "2020-09-03 16:14:45 WARNING  # tokens (including EOS '.'): 23340\n",
            "2020-09-03 16:14:45 WARNING  Token-level (P)PPL: 72326.08524453637\n",
            "2020-09-03 16:14:45 WARNING  Word-normalized (P)PPL: 1349771.396399468\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 16:14:48.766143: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 16:14:55 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 16:14:55 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 16:14:55 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 16:14:55 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 16:15:00 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "2020-09-03 16:15:00 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 16:15:00 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 16:15:00 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 16:15:00 INFO     loading file None\n",
            "2020-09-03 16:15:00 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 16:15:00 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 16:15:00 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 16:15:04 WARNING  # sentences: 100\n",
            "2020-09-03 16:15:04 INFO     FixedBucketSampler:\n",
            "  sample_num=17363, batch_num=37\n",
            "  key=[174, 175, 176, 177, 178]\n",
            "  cnt=[1892, 6747, 5742, 1750, 1232]\n",
            "  batch_size=[511, 508, 505, 502, 500]\n",
            "[16:15:04] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 16:17:12 INFO     9232 sents of 17363, batch 20 of 37\n",
            "2020-09-03 16:19:05 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 16:19:05 WARNING  # words (excluding EOS '.'): 14157\n",
            "2020-09-03 16:19:05 WARNING  longest sentence: 143\n",
            "2020-09-03 16:19:05 WARNING  # tokens (including EOS '.'): 17363\n",
            "2020-09-03 16:19:05 WARNING  Token-level (P)PPL: 36008.54774132649\n",
            "2020-09-03 16:19:05 WARNING  Word-normalized (P)PPL: 387481.59617796284\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 16:19:08.504778: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 16:19:15 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 16:19:15 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 16:19:15 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 16:19:15 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 16:19:20 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
            "2020-09-03 16:19:20 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 16:19:20 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 16:19:20 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 16:19:20 INFO     loading file None\n",
            "2020-09-03 16:19:20 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 16:19:20 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 16:19:20 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 16:19:24 WARNING  # sentences: 100\n",
            "2020-09-03 16:19:24 INFO     FixedBucketSampler:\n",
            "  sample_num=19637, batch_num=41\n",
            "  key=[197, 198, 199, 200]\n",
            "  cnt=[2730, 8428, 6895, 1584]\n",
            "  batch_size=[507, 505, 502, 500]\n",
            "[16:19:24] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 16:21:54 INFO     9489 sents of 19637, batch 20 of 41\n",
            "2020-09-03 16:24:34 INFO     19442 sents of 19637, batch 40 of 41\n",
            "2020-09-03 16:24:37 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 16:24:37 WARNING  # words (excluding EOS '.'): 15748\n",
            "2020-09-03 16:24:37 WARNING  longest sentence: 158\n",
            "2020-09-03 16:24:37 WARNING  # tokens (including EOS '.'): 19637\n",
            "2020-09-03 16:24:37 WARNING  Token-level (P)PPL: 64652.6773283153\n",
            "2020-09-03 16:24:37 WARNING  Word-normalized (P)PPL: 996713.7410616039\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 16:24:40.618548: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 16:24:47 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 16:24:47 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 16:24:47 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 16:24:47 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 16:24:52 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "2020-09-03 16:24:52 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 16:24:52 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 16:24:52 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 16:24:52 INFO     loading file None\n",
            "2020-09-03 16:24:52 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 16:24:52 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 16:24:52 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 16:24:56 WARNING  # sentences: 100\n",
            "2020-09-03 16:24:56 INFO     FixedBucketSampler:\n",
            "  sample_num=15409, batch_num=33\n",
            "  key=[154, 155, 156, 157, 158]\n",
            "  cnt=[1976, 3060, 3850, 4495, 2028]\n",
            "  batch_size=[512, 509, 506, 503, 500]\n",
            "[16:24:56] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 16:26:54 INFO     9559 sents of 15409, batch 20 of 33\n",
            "2020-09-03 16:28:06 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 16:28:06 WARNING  # words (excluding EOS '.'): 13058\n",
            "2020-09-03 16:28:06 WARNING  longest sentence: 130\n",
            "2020-09-03 16:28:06 WARNING  # tokens (including EOS '.'): 15409\n",
            "2020-09-03 16:28:06 WARNING  Token-level (P)PPL: 92772.25289325336\n",
            "2020-09-03 16:28:06 WARNING  Word-normalized (P)PPL: 727388.4360546322\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 16:28:09.324755: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 16:28:16 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 16:28:16 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 16:28:16 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 16:28:16 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 16:28:20 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "2020-09-03 16:28:20 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 16:28:20 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 16:28:20 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 16:28:20 INFO     loading file None\n",
            "2020-09-03 16:28:20 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 16:28:20 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 16:28:21 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 16:28:25 WARNING  # sentences: 100\n",
            "2020-09-03 16:28:25 INFO     FixedBucketSampler:\n",
            "  sample_num=21908, batch_num=45\n",
            "  key=[219, 220, 221, 222, 223]\n",
            "  cnt=[434, 5450, 8760, 6380, 884]\n",
            "  batch_size=[509, 506, 504, 502, 500]\n",
            "[16:28:25] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 16:31:19 INFO     9784 sents of 21908, batch 20 of 45\n",
            "2020-09-03 16:34:16 INFO     19566 sents of 21908, batch 40 of 45\n",
            "2020-09-03 16:34:58 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 16:34:58 WARNING  # words (excluding EOS '.'): 16492\n",
            "2020-09-03 16:34:58 WARNING  longest sentence: 165\n",
            "2020-09-03 16:34:58 WARNING  # tokens (including EOS '.'): 21908\n",
            "2020-09-03 16:34:58 WARNING  Token-level (P)PPL: 75505.78467149178\n",
            "2020-09-03 16:34:58 WARNING  Word-normalized (P)PPL: 3019374.45426414\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 16:35:01.264789: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 16:35:08 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 16:35:08 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 16:35:08 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 16:35:08 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 16:35:13 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "2020-09-03 16:35:13 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 16:35:13 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 16:35:13 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 16:35:13 INFO     loading file None\n",
            "2020-09-03 16:35:13 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 16:35:13 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 16:35:13 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 16:35:17 WARNING  # sentences: 100\n",
            "2020-09-03 16:35:17 INFO     FixedBucketSampler:\n",
            "  sample_num=26440, batch_num=55\n",
            "  key=[264, 265, 266, 267, 268]\n",
            "  cnt=[1572, 4997, 5280, 10335, 4256]\n",
            "  batch_size=[507, 505, 503, 501, 500]\n",
            "[16:35:17] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 16:38:58 INFO     9767 sents of 26440, batch 20 of 55\n",
            "2020-09-03 16:42:44 INFO     19621 sents of 26440, batch 40 of 55\n",
            "2020-09-03 16:45:18 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 16:45:18 WARNING  # words (excluding EOS '.'): 22568\n",
            "2020-09-03 16:45:18 WARNING  longest sentence: 225\n",
            "2020-09-03 16:45:18 WARNING  # tokens (including EOS '.'): 26440\n",
            "2020-09-03 16:45:18 WARNING  Token-level (P)PPL: 88585.8739922518\n",
            "2020-09-03 16:45:18 WARNING  Word-normalized (P)PPL: 625439.9286294266\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 16:45:22.054683: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 16:45:29 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 16:45:29 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 16:45:29 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 16:45:29 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 16:45:33 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "2020-09-03 16:45:33 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 16:45:33 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 16:45:33 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 16:45:33 INFO     loading file None\n",
            "2020-09-03 16:45:33 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 16:45:33 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 16:45:34 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 16:45:37 WARNING  # sentences: 100\n",
            "2020-09-03 16:45:38 INFO     FixedBucketSampler:\n",
            "  sample_num=18021, batch_num=37\n",
            "  key=[180, 181, 182, 183, 184]\n",
            "  cnt=[890, 3043, 7560, 4344, 2184]\n",
            "  batch_size=[511, 508, 505, 502, 500]\n",
            "[16:45:38] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 16:47:55 INFO     9558 sents of 18021, batch 20 of 37\n",
            "2020-09-03 16:49:58 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 16:49:58 WARNING  # words (excluding EOS '.'): 12492\n",
            "2020-09-03 16:49:58 WARNING  longest sentence: 125\n",
            "2020-09-03 16:49:58 WARNING  # tokens (including EOS '.'): 18021\n",
            "2020-09-03 16:49:58 WARNING  Token-level (P)PPL: 77750.37742781438\n",
            "2020-09-03 16:49:58 WARNING  Word-normalized (P)PPL: 11359086.546814825\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 16:50:01.436852: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 16:50:08 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 16:50:08 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 16:50:08 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 16:50:08 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 16:50:13 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
            "2020-09-03 16:50:13 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 16:50:13 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 16:50:13 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 16:50:13 INFO     loading file None\n",
            "2020-09-03 16:50:13 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 16:50:13 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 16:50:13 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 16:50:17 WARNING  # sentences: 100\n",
            "2020-09-03 16:50:17 INFO     FixedBucketSampler:\n",
            "  sample_num=14203, batch_num=30\n",
            "  key=[142, 143, 144, 145, 146]\n",
            "  cnt=[560, 3243, 5964, 4004, 432]\n",
            "  batch_size=[514, 510, 506, 503, 500]\n",
            "[16:50:17] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 16:52:07 INFO     10002 sents of 14203, batch 20 of 30\n",
            "2020-09-03 16:52:55 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 16:52:55 WARNING  # words (excluding EOS '.'): 11831\n",
            "2020-09-03 16:52:55 WARNING  longest sentence: 119\n",
            "2020-09-03 16:52:55 WARNING  # tokens (including EOS '.'): 14203\n",
            "2020-09-03 16:52:55 WARNING  Token-level (P)PPL: 124664.39040705664\n",
            "2020-09-03 16:52:55 WARNING  Word-normalized (P)PPL: 1310355.1664089772\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 16:52:58.542864: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 16:53:05 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 16:53:05 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 16:53:05 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 16:53:05 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 16:53:10 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
            "2020-09-03 16:53:10 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 16:53:10 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 16:53:10 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 16:53:10 INFO     loading file None\n",
            "2020-09-03 16:53:10 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 16:53:10 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 16:53:10 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 16:53:14 WARNING  # sentences: 100\n",
            "2020-09-03 16:53:14 INFO     FixedBucketSampler:\n",
            "  sample_num=17084, batch_num=36\n",
            "  key=[172, 173, 174]\n",
            "  cnt=[4590, 10602, 1892]\n",
            "  batch_size=[505, 502, 500]\n",
            "[16:53:14] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 16:55:28 INFO     9924 sents of 17084, batch 20 of 36\n",
            "2020-09-03 16:57:06 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 16:57:06 WARNING  # words (excluding EOS '.'): 12159\n",
            "2020-09-03 16:57:06 WARNING  longest sentence: 122\n",
            "2020-09-03 16:57:06 WARNING  # tokens (including EOS '.'): 17084\n",
            "2020-09-03 16:57:06 WARNING  Token-level (P)PPL: 78241.50672551821\n",
            "2020-09-03 16:57:06 WARNING  Word-normalized (P)PPL: 7507973.964924737\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 16:57:10.076136: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 16:57:17 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 16:57:17 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 16:57:17 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 16:57:17 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 16:57:21 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "2020-09-03 16:57:21 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 16:57:21 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 16:57:21 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 16:57:21 INFO     loading file None\n",
            "2020-09-03 16:57:21 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 16:57:21 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 16:57:21 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 16:57:25 WARNING  # sentences: 100\n",
            "2020-09-03 16:57:26 INFO     FixedBucketSampler:\n",
            "  sample_num=27953, batch_num=57\n",
            "  key=[280, 281, 282, 283]\n",
            "  cnt=[1390, 12555, 11760, 2248]\n",
            "  batch_size=[505, 503, 501, 500]\n",
            "[16:57:26] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 17:01:22 INFO     9763 sents of 27953, batch 20 of 57\n",
            "2020-09-03 17:05:20 INFO     19541 sents of 27953, batch 40 of 57\n",
            "2020-09-03 17:08:45 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 17:08:45 WARNING  # words (excluding EOS '.'): 23433\n",
            "2020-09-03 17:08:45 WARNING  longest sentence: 235\n",
            "2020-09-03 17:08:45 WARNING  # tokens (including EOS '.'): 27953\n",
            "2020-09-03 17:08:45 WARNING  Token-level (P)PPL: 64980.53018831698\n",
            "2020-09-03 17:08:45 WARNING  Word-normalized (P)PPL: 550963.4433312282\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 17:08:48.412790: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 17:08:55 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 17:08:55 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 17:08:55 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 17:08:55 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 17:09:00 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
            "2020-09-03 17:09:00 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 17:09:00 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 17:09:00 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 17:09:00 INFO     loading file None\n",
            "2020-09-03 17:09:00 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 17:09:00 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 17:09:00 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 17:09:04 WARNING  # sentences: 100\n",
            "2020-09-03 17:09:04 INFO     FixedBucketSampler:\n",
            "  sample_num=16020, batch_num=33\n",
            "  key=[160, 161, 162, 163]\n",
            "  cnt=[316, 2385, 7040, 6279]\n",
            "  batch_size=[509, 506, 503, 500]\n",
            "[17:09:04] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 17:11:09 INFO     9800 sents of 16020, batch 20 of 33\n",
            "2020-09-03 17:12:29 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 17:12:29 WARNING  # words (excluding EOS '.'): 11723\n",
            "2020-09-03 17:12:29 WARNING  longest sentence: 117\n",
            "2020-09-03 17:12:29 WARNING  # tokens (including EOS '.'): 16020\n",
            "2020-09-03 17:12:29 WARNING  Token-level (P)PPL: 48187.71922211299\n",
            "2020-09-03 17:12:29 WARNING  Word-normalized (P)PPL: 2508652.3054286586\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 17:12:32.165492: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 17:12:39 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 17:12:39 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 17:12:39 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 17:12:39 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 17:12:43 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
            "2020-09-03 17:12:43 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 17:12:43 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 17:12:43 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 17:12:43 INFO     loading file None\n",
            "2020-09-03 17:12:43 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 17:12:43 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 17:12:43 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 17:12:47 WARNING  # sentences: 100\n",
            "2020-09-03 17:12:48 INFO     FixedBucketSampler:\n",
            "  sample_num=16845, batch_num=37\n",
            "  key=[169, 170, 171, 172, 173]\n",
            "  cnt=[668, 10248, 3718, 2040, 171]\n",
            "  batch_size=[511, 508, 505, 502, 500]\n",
            "[17:12:48] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 17:14:48 INFO     8977 sents of 16845, batch 20 of 37\n",
            "2020-09-03 17:16:35 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 17:16:35 WARNING  # words (excluding EOS '.'): 12922\n",
            "2020-09-03 17:16:35 WARNING  longest sentence: 130\n",
            "2020-09-03 17:16:35 WARNING  # tokens (including EOS '.'): 16845\n",
            "2020-09-03 17:16:35 WARNING  Token-level (P)PPL: 133774.53615484884\n",
            "2020-09-03 17:16:35 WARNING  Word-normalized (P)PPL: 4816073.937915429\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 17:16:38.119044: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 17:16:45 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 17:16:45 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 17:16:45 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 17:16:45 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 17:16:50 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias']\n",
            "2020-09-03 17:16:50 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 17:16:50 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 17:16:50 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 17:16:50 INFO     loading file None\n",
            "2020-09-03 17:16:50 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 17:16:50 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 17:16:50 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 17:16:54 WARNING  # sentences: 100\n",
            "2020-09-03 17:16:54 INFO     FixedBucketSampler:\n",
            "  sample_num=20629, batch_num=42\n",
            "  key=[207, 208, 209, 210]\n",
            "  cnt=[2255, 10506, 7452, 416]\n",
            "  batch_size=[507, 504, 502, 500]\n",
            "[17:16:54] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 17:19:41 INFO     9884 sents of 20629, batch 20 of 42\n",
            "2020-09-03 17:22:29 INFO     19895 sents of 20629, batch 40 of 42\n",
            "2020-09-03 17:22:41 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 17:22:41 WARNING  # words (excluding EOS '.'): 14423\n",
            "2020-09-03 17:22:41 WARNING  longest sentence: 144\n",
            "2020-09-03 17:22:41 WARNING  # tokens (including EOS '.'): 20629\n",
            "2020-09-03 17:22:41 WARNING  Token-level (P)PPL: 100435.06487798918\n",
            "2020-09-03 17:22:41 WARNING  Word-normalized (P)PPL: 14260062.074351551\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 17:22:44.401738: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 17:22:51 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 17:22:51 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 17:22:51 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 17:22:51 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 17:22:56 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
            "2020-09-03 17:22:56 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 17:22:56 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 17:22:56 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 17:22:56 INFO     loading file None\n",
            "2020-09-03 17:22:56 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 17:22:56 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 17:22:56 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 17:23:00 WARNING  # sentences: 100\n",
            "2020-09-03 17:23:00 INFO     FixedBucketSampler:\n",
            "  sample_num=18125, batch_num=38\n",
            "  key=[181, 182, 183, 184, 185]\n",
            "  cnt=[716, 3600, 5973, 6006, 1830]\n",
            "  batch_size=[511, 508, 505, 502, 500]\n",
            "[17:23:00] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 17:25:23 INFO     9856 sents of 18125, batch 20 of 38\n",
            "2020-09-03 17:27:23 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 17:27:23 WARNING  # words (excluding EOS '.'): 12826\n",
            "2020-09-03 17:27:23 WARNING  longest sentence: 128\n",
            "2020-09-03 17:27:23 WARNING  # tokens (including EOS '.'): 18125\n",
            "2020-09-03 17:27:23 WARNING  Token-level (P)PPL: 61930.74318284709\n",
            "2020-09-03 17:27:23 WARNING  Word-normalized (P)PPL: 5910976.7306346875\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 17:27:26.444628: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 17:27:33 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 17:27:33 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 17:27:33 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 17:27:33 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 17:27:38 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "2020-09-03 17:27:38 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 17:27:38 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 17:27:38 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 17:27:38 INFO     loading file None\n",
            "2020-09-03 17:27:38 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 17:27:38 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 17:27:38 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 17:27:42 WARNING  # sentences: 100\n",
            "2020-09-03 17:27:42 INFO     FixedBucketSampler:\n",
            "  sample_num=20147, batch_num=42\n",
            "  key=[202, 203, 204, 205]\n",
            "  cnt=[3800, 7035, 5252, 4060]\n",
            "  batch_size=[507, 504, 502, 500]\n",
            "[17:27:42] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 17:30:15 INFO     9312 sents of 20147, batch 20 of 42\n",
            "2020-09-03 17:33:00 INFO     19389 sents of 20147, batch 40 of 42\n",
            "2020-09-03 17:33:12 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 17:33:12 WARNING  # words (excluding EOS '.'): 16215\n",
            "2020-09-03 17:33:12 WARNING  longest sentence: 162\n",
            "2020-09-03 17:33:12 WARNING  # tokens (including EOS '.'): 20147\n",
            "2020-09-03 17:33:12 WARNING  Token-level (P)PPL: 59647.80535159919\n",
            "2020-09-03 17:33:12 WARNING  Word-normalized (P)PPL: 858293.1209797574\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 17:33:15.715230: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 17:33:22 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 17:33:22 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 17:33:22 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 17:33:22 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 17:33:27 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "2020-09-03 17:33:27 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 17:33:27 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 17:33:27 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 17:33:27 INFO     loading file None\n",
            "2020-09-03 17:33:27 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 17:33:27 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 17:33:27 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 17:33:31 WARNING  # sentences: 100\n",
            "2020-09-03 17:33:31 INFO     FixedBucketSampler:\n",
            "  sample_num=18446, batch_num=39\n",
            "  key=[185, 186, 187, 188, 189, 190]\n",
            "  cnt=[2379, 7544, 6660, 1488, 187, 188]\n",
            "  batch_size=[513, 510, 508, 505, 502, 500]\n",
            "[17:33:31] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 17:35:44 INFO     9033 sents of 18446, batch 20 of 39\n",
            "2020-09-03 17:38:04 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 17:38:04 WARNING  # words (excluding EOS '.'): 14356\n",
            "2020-09-03 17:38:04 WARNING  longest sentence: 144\n",
            "2020-09-03 17:38:04 WARNING  # tokens (including EOS '.'): 18446\n",
            "2020-09-03 17:38:04 WARNING  Token-level (P)PPL: 110814.9131462199\n",
            "2020-09-03 17:38:04 WARNING  Word-normalized (P)PPL: 3032464.0888939747\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 17:38:07.292991: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 17:38:14 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 17:38:14 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 17:38:14 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 17:38:14 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 17:38:18 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias']\n",
            "2020-09-03 17:38:18 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 17:38:18 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 17:38:18 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 17:38:18 INFO     loading file None\n",
            "2020-09-03 17:38:18 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 17:38:18 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 17:38:19 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 17:38:23 WARNING  # sentences: 100\n",
            "2020-09-03 17:38:23 INFO     FixedBucketSampler:\n",
            "  sample_num=36658, batch_num=76\n",
            "  key=[366, 367, 368, 369, 370, 371]\n",
            "  cnt=[728, 5110, 10980, 12845, 5888, 1107]\n",
            "  batch_size=[506, 505, 504, 502, 501, 500]\n",
            "[17:38:23] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 17:43:32 INFO     9505 sents of 36658, batch 20 of 76\n",
            "2020-09-03 17:48:59 INFO     19545 sents of 36658, batch 40 of 76\n",
            "2020-09-03 17:54:17 INFO     29416 sents of 36658, batch 60 of 76\n",
            "2020-09-03 17:58:12 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 17:58:12 WARNING  # words (excluding EOS '.'): 27652\n",
            "2020-09-03 17:58:12 WARNING  longest sentence: 278\n",
            "2020-09-03 17:58:12 WARNING  # tokens (including EOS '.'): 36658\n",
            "2020-09-03 17:58:12 WARNING  Token-level (P)PPL: 88250.70682879053\n",
            "2020-09-03 17:58:12 WARNING  Word-normalized (P)PPL: 3601577.372728477\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 17:58:15.983423: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 17:58:22 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 17:58:22 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 17:58:22 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 17:58:22 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 17:58:27 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias']\n",
            "2020-09-03 17:58:27 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 17:58:27 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 17:58:27 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 17:58:27 INFO     loading file None\n",
            "2020-09-03 17:58:27 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 17:58:27 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 17:58:27 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 17:58:31 WARNING  # sentences: 100\n",
            "2020-09-03 17:58:32 INFO     FixedBucketSampler:\n",
            "  sample_num=38209, batch_num=79\n",
            "  key=[383, 384, 385]\n",
            "  cnt=[4572, 25594, 8043]\n",
            "  batch_size=[502, 501, 500]\n",
            "[17:58:32] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 18:03:59 INFO     9546 sents of 38209, batch 20 of 79\n",
            "2020-09-03 18:09:36 INFO     19566 sents of 38209, batch 40 of 79\n",
            "2020-09-03 18:15:14 INFO     29586 sents of 38209, batch 60 of 79\n",
            "2020-09-03 18:20:06 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 18:20:06 WARNING  # words (excluding EOS '.'): 28125\n",
            "2020-09-03 18:20:06 WARNING  longest sentence: 281\n",
            "2020-09-03 18:20:06 WARNING  # tokens (including EOS '.'): 38209\n",
            "2020-09-03 18:20:06 WARNING  Token-level (P)PPL: 98566.95835188053\n",
            "2020-09-03 18:20:06 WARNING  Word-normalized (P)PPL: 6084079.666763845\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 18:20:09.699169: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 18:20:16 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 18:20:16 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 18:20:16 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 18:20:16 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 18:20:21 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
            "2020-09-03 18:20:21 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 18:20:21 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 18:20:21 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 18:20:21 INFO     loading file None\n",
            "2020-09-03 18:20:21 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 18:20:21 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 18:20:21 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 18:20:25 WARNING  # sentences: 100\n",
            "/usr/local/lib/python3.6/dist-packages/gluonnlp/data/sampler.py:354: UserWarning: Some buckets are empty and will be removed. Unused bucket keys=[158]\n",
            "  str(unused_bucket_keys))\n",
            "2020-09-03 18:20:25 INFO     FixedBucketSampler:\n",
            "  sample_num=15414, batch_num=33\n",
            "  key=[154, 155, 156, 157, 159, 160]\n",
            "  cnt=[304, 4590, 8008, 620, 628, 1264]\n",
            "  batch_size=[519, 516, 512, 509, 503, 500]\n",
            "[18:20:25] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 18:22:17 INFO     9168 sents of 15414, batch 20 of 33\n",
            "2020-09-03 18:23:34 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 18:23:34 WARNING  # words (excluding EOS '.'): 12674\n",
            "2020-09-03 18:23:34 WARNING  longest sentence: 127\n",
            "2020-09-03 18:23:34 WARNING  # tokens (including EOS '.'): 15414\n",
            "2020-09-03 18:23:34 WARNING  Token-level (P)PPL: 82247.27419803731\n",
            "2020-09-03 18:23:34 WARNING  Word-normalized (P)PPL: 950002.4229230756\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 18:23:37.833265: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 18:23:44 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 18:23:44 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 18:23:44 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 18:23:44 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 18:23:49 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight']\n",
            "2020-09-03 18:23:49 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 18:23:49 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 18:23:49 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 18:23:49 INFO     loading file None\n",
            "2020-09-03 18:23:49 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 18:23:49 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 18:23:49 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 18:23:53 WARNING  # sentences: 100\n",
            "2020-09-03 18:23:54 INFO     FixedBucketSampler:\n",
            "  sample_num=19771, batch_num=41\n",
            "  key=[198, 199, 200, 201, 202]\n",
            "  cnt=[784, 7486, 8316, 2985, 200]\n",
            "  batch_size=[510, 507, 505, 502, 500]\n",
            "[18:23:54] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 18:26:29 INFO     9750 sents of 19771, batch 20 of 41\n",
            "2020-09-03 18:29:07 INFO     19497 sents of 19771, batch 40 of 41\n",
            "2020-09-03 18:29:11 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 18:29:11 WARNING  # words (excluding EOS '.'): 14336\n",
            "2020-09-03 18:29:11 WARNING  longest sentence: 144\n",
            "2020-09-03 18:29:11 WARNING  # tokens (including EOS '.'): 19771\n",
            "2020-09-03 18:29:11 WARNING  Token-level (P)PPL: 100059.79931680975\n",
            "2020-09-03 18:29:11 WARNING  Word-normalized (P)PPL: 7869291.323085179\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 18:29:14.669471: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 18:29:21 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 18:29:21 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 18:29:21 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 18:29:21 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 18:29:26 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
            "2020-09-03 18:29:26 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 18:29:26 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 18:29:26 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 18:29:26 INFO     loading file None\n",
            "2020-09-03 18:29:26 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 18:29:26 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 18:29:26 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 18:29:30 WARNING  # sentences: 100\n",
            "2020-09-03 18:29:30 INFO     FixedBucketSampler:\n",
            "  sample_num=16683, batch_num=37\n",
            "  key=[166, 167, 168, 169, 170, 171, 172]\n",
            "  cnt=[656, 1980, 5478, 3173, 3192, 1014, 1190]\n",
            "  batch_size=[518, 514, 511, 508, 505, 502, 500]\n",
            "[18:29:30] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 18:31:24 INFO     8569 sents of 16683, batch 20 of 37\n",
            "2020-09-03 18:33:13 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 18:33:13 WARNING  # words (excluding EOS '.'): 13236\n",
            "2020-09-03 18:33:13 WARNING  longest sentence: 134\n",
            "2020-09-03 18:33:13 WARNING  # tokens (including EOS '.'): 16683\n",
            "2020-09-03 18:33:13 WARNING  Token-level (P)PPL: 100852.38414509538\n",
            "2020-09-03 18:33:13 WARNING  Word-normalized (P)PPL: 2026640.4061899604\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 18:33:16.319015: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 18:33:23 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 18:33:23 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 18:33:23 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 18:33:23 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 18:33:27 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
            "2020-09-03 18:33:27 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 18:33:27 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 18:33:27 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 18:33:27 INFO     loading file None\n",
            "2020-09-03 18:33:27 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 18:33:27 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 18:33:28 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 18:33:32 WARNING  # sentences: 100\n",
            "2020-09-03 18:33:32 INFO     FixedBucketSampler:\n",
            "  sample_num=30559, batch_num=62\n",
            "  key=[307, 308]\n",
            "  cnt=[12505, 18054]\n",
            "  batch_size=[501, 500]\n",
            "[18:33:32] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 18:37:59 INFO     10000 sents of 30559, batch 20 of 62\n",
            "2020-09-03 18:42:15 INFO     19557 sents of 30559, batch 40 of 62\n",
            "2020-09-03 18:46:44 INFO     29577 sents of 30559, batch 60 of 62\n",
            "2020-09-03 18:47:11 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 18:47:11 WARNING  # words (excluding EOS '.'): 23632\n",
            "2020-09-03 18:47:11 WARNING  longest sentence: 236\n",
            "2020-09-03 18:47:11 WARNING  # tokens (including EOS '.'): 30559\n",
            "2020-09-03 18:47:11 WARNING  Token-level (P)PPL: 95352.3093957848\n",
            "2020-09-03 18:47:11 WARNING  Word-normalized (P)PPL: 2747073.5493464824\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 18:47:14.349045: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 18:47:21 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 18:47:21 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 18:47:21 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 18:47:21 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 18:47:26 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
            "2020-09-03 18:47:26 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 18:47:26 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 18:47:26 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 18:47:26 INFO     loading file None\n",
            "2020-09-03 18:47:26 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 18:47:26 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 18:47:26 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 18:47:30 WARNING  # sentences: 100\n",
            "2020-09-03 18:47:30 INFO     FixedBucketSampler:\n",
            "  sample_num=30892, batch_num=65\n",
            "  key=[309, 310, 311, 312, 313]\n",
            "  cnt=[2149, 8316, 11742, 7130, 1555]\n",
            "  batch_size=[506, 504, 503, 501, 500]\n",
            "[18:47:30] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 18:51:37 INFO     9188 sents of 30892, batch 20 of 65\n",
            "2020-09-03 18:56:10 INFO     19248 sents of 30892, batch 40 of 65\n",
            "2020-09-03 19:00:26 INFO     28743 sents of 30892, batch 60 of 65\n",
            "2020-09-03 19:01:24 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 19:01:24 WARNING  # words (excluding EOS '.'): 24512\n",
            "2020-09-03 19:01:24 WARNING  longest sentence: 246\n",
            "2020-09-03 19:01:24 WARNING  # tokens (including EOS '.'): 30892\n",
            "2020-09-03 19:01:24 WARNING  Token-level (P)PPL: 107148.66699963939\n",
            "2020-09-03 19:01:24 WARNING  Word-normalized (P)PPL: 2183710.911927116\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 19:01:27.465549: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 19:01:34 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 19:01:34 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 19:01:34 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 19:01:34 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 19:01:39 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
            "2020-09-03 19:01:39 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 19:01:39 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 19:01:39 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 19:01:39 INFO     loading file None\n",
            "2020-09-03 19:01:39 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 19:01:39 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 19:01:39 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 19:01:43 WARNING  # sentences: 100\n",
            "2020-09-03 19:01:43 INFO     FixedBucketSampler:\n",
            "  sample_num=33170, batch_num=68\n",
            "  key=[332, 333, 334, 335, 336]\n",
            "  cnt=[5940, 9930, 7968, 6660, 2672]\n",
            "  batch_size=[506, 504, 502, 501, 500]\n",
            "[19:01:43] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 19:06:13 INFO     9332 sents of 33170, batch 20 of 68\n",
            "2020-09-03 19:11:05 INFO     19316 sents of 33170, batch 40 of 68\n",
            "2020-09-03 19:15:54 INFO     29254 sents of 33170, batch 60 of 68\n",
            "2020-09-03 19:17:48 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 19:17:48 WARNING  # words (excluding EOS '.'): 22772\n",
            "2020-09-03 19:17:48 WARNING  longest sentence: 228\n",
            "2020-09-03 19:17:48 WARNING  # tokens (including EOS '.'): 33170\n",
            "2020-09-03 19:17:48 WARNING  Token-level (P)PPL: 87725.87420515728\n",
            "2020-09-03 19:17:48 WARNING  Word-normalized (P)PPL: 15857180.794052446\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 19:17:51.485177: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 19:17:58 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 19:17:58 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 19:17:58 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 19:17:58 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 19:18:03 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "2020-09-03 19:18:03 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 19:18:03 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 19:18:03 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 19:18:03 INFO     loading file None\n",
            "2020-09-03 19:18:03 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 19:18:03 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 19:18:03 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 19:18:07 WARNING  # sentences: 100\n",
            "2020-09-03 19:18:07 INFO     FixedBucketSampler:\n",
            "  sample_num=17252, batch_num=36\n",
            "  key=[173, 174, 175, 176]\n",
            "  cnt=[2223, 6192, 6401, 2436]\n",
            "  batch_size=[508, 505, 502, 500]\n",
            "[19:18:07] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 19:20:22 INFO     9847 sents of 17252, batch 20 of 36\n",
            "2020-09-03 19:22:04 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 19:22:04 WARNING  # words (excluding EOS '.'): 13897\n",
            "2020-09-03 19:22:04 WARNING  longest sentence: 140\n",
            "2020-09-03 19:22:04 WARNING  # tokens (including EOS '.'): 17252\n",
            "2020-09-03 19:22:04 WARNING  Token-level (P)PPL: 58938.43385523911\n",
            "2020-09-03 19:22:04 WARNING  Word-normalized (P)PPL: 835725.2094865349\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 19:22:07.582075: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 19:22:14 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 19:22:14 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 19:22:14 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 19:22:14 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 19:22:19 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
            "2020-09-03 19:22:19 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 19:22:19 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 19:22:19 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 19:22:19 INFO     loading file None\n",
            "2020-09-03 19:22:19 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 19:22:19 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 19:22:19 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 19:22:23 WARNING  # sentences: 100\n",
            "2020-09-03 19:22:23 INFO     FixedBucketSampler:\n",
            "  sample_num=17588, batch_num=39\n",
            "  key=[174, 175, 176, 177, 178, 179, 180, 181]\n",
            "  cnt=[172, 692, 1740, 3850, 4752, 4779, 1424, 179]\n",
            "  batch_size=[520, 517, 514, 511, 508, 505, 502, 500]\n",
            "[19:22:23] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 19:24:35 INFO     9430 sents of 17588, batch 20 of 39\n",
            "2020-09-03 19:26:29 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 19:26:29 WARNING  # words (excluding EOS '.'): 12141\n",
            "2020-09-03 19:26:29 WARNING  longest sentence: 122\n",
            "2020-09-03 19:26:29 WARNING  # tokens (including EOS '.'): 17588\n",
            "2020-09-03 19:26:29 WARNING  Token-level (P)PPL: 82133.77532273797\n",
            "2020-09-03 19:26:29 WARNING  Word-normalized (P)PPL: 13164302.082463901\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 19:26:32.276435: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 19:26:39 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 19:26:39 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 19:26:39 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 19:26:39 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 19:26:44 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
            "2020-09-03 19:26:44 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 19:26:44 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 19:26:44 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 19:26:44 INFO     loading file None\n",
            "2020-09-03 19:26:44 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 19:26:44 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 19:26:44 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 19:26:48 WARNING  # sentences: 100\n",
            "2020-09-03 19:26:48 INFO     FixedBucketSampler:\n",
            "  sample_num=17815, batch_num=39\n",
            "  key=[177, 178, 179, 180, 181, 182, 183, 184]\n",
            "  cnt=[350, 1584, 4071, 4806, 3938, 2160, 724, 182]\n",
            "  batch_size=[519, 516, 513, 511, 508, 505, 502, 500]\n",
            "[19:26:48] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 19:28:56 INFO     9048 sents of 17815, batch 20 of 39\n",
            "2020-09-03 19:31:00 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 19:31:00 WARNING  # words (excluding EOS '.'): 12880\n",
            "2020-09-03 19:31:00 WARNING  longest sentence: 130\n",
            "2020-09-03 19:31:00 WARNING  # tokens (including EOS '.'): 17815\n",
            "2020-09-03 19:31:00 WARNING  Token-level (P)PPL: 83380.32665831609\n",
            "2020-09-03 19:31:00 WARNING  Word-normalized (P)PPL: 6405894.848828108\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 19:31:04.062120: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 19:31:11 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 19:31:11 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 19:31:11 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 19:31:11 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 19:31:15 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
            "2020-09-03 19:31:15 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 19:31:15 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 19:31:15 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 19:31:15 INFO     loading file None\n",
            "2020-09-03 19:31:15 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 19:31:15 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 19:31:15 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 19:31:19 WARNING  # sentences: 100\n",
            "2020-09-03 19:31:20 INFO     FixedBucketSampler:\n",
            "  sample_num=15970, batch_num=35\n",
            "  key=[159, 160, 161, 162, 163, 164]\n",
            "  cnt=[314, 1264, 4611, 6560, 3059, 162]\n",
            "  batch_size=[515, 512, 509, 506, 503, 500]\n",
            "[19:31:20] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 19:33:17 INFO     9293 sents of 15970, batch 20 of 35\n",
            "2020-09-03 19:34:42 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 19:34:42 WARNING  # words (excluding EOS '.'): 12220\n",
            "2020-09-03 19:34:42 WARNING  longest sentence: 123\n",
            "2020-09-03 19:34:42 WARNING  # tokens (including EOS '.'): 15970\n",
            "2020-09-03 19:34:42 WARNING  Token-level (P)PPL: 65216.50768437661\n",
            "2020-09-03 19:34:42 WARNING  Word-normalized (P)PPL: 1957756.0911787802\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 19:34:46.041503: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 19:34:53 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 19:34:53 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 19:34:53 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 19:34:53 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 19:34:57 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
            "2020-09-03 19:34:57 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 19:34:57 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 19:34:57 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 19:34:57 INFO     loading file None\n",
            "2020-09-03 19:34:57 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 19:34:57 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 19:34:57 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 19:35:01 WARNING  # sentences: 100\n",
            "2020-09-03 19:35:02 INFO     FixedBucketSampler:\n",
            "  sample_num=20006, batch_num=44\n",
            "  key=[200, 201, 202, 203, 204, 205]\n",
            "  cnt=[1782, 5174, 6200, 4221, 2020, 609]\n",
            "  batch_size=[512, 509, 507, 504, 502, 500]\n",
            "[19:35:02] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 19:37:26 INFO     8878 sents of 20006, batch 20 of 44\n",
            "2020-09-03 19:39:59 INFO     18224 sents of 20006, batch 40 of 44\n",
            "2020-09-03 19:40:27 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 19:40:27 WARNING  # words (excluding EOS '.'): 14614\n",
            "2020-09-03 19:40:27 WARNING  longest sentence: 147\n",
            "2020-09-03 19:40:27 WARNING  # tokens (including EOS '.'): 20006\n",
            "2020-09-03 19:40:27 WARNING  Token-level (P)PPL: 73570.4843647043\n",
            "2020-09-03 19:40:27 WARNING  Word-normalized (P)PPL: 4595457.584771738\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 19:40:30.881956: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 19:40:37 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 19:40:37 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 19:40:37 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 19:40:37 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 19:40:42 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
            "2020-09-03 19:40:42 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 19:40:42 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 19:40:42 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 19:40:42 INFO     loading file None\n",
            "2020-09-03 19:40:42 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 19:40:42 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 19:40:42 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 19:40:46 WARNING  # sentences: 100\n",
            "2020-09-03 19:40:47 INFO     FixedBucketSampler:\n",
            "  sample_num=23492, batch_num=49\n",
            "  key=[236, 237, 238]\n",
            "  cnt=[5850, 13630, 4012]\n",
            "  batch_size=[504, 502, 500]\n",
            "[19:40:47] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 19:43:50 INFO     9534 sents of 23492, batch 20 of 49\n",
            "2020-09-03 19:46:57 INFO     19154 sents of 23492, batch 40 of 49\n",
            "2020-09-03 19:48:22 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 19:48:22 WARNING  # words (excluding EOS '.'): 16125\n",
            "2020-09-03 19:48:22 WARNING  longest sentence: 161\n",
            "2020-09-03 19:48:22 WARNING  # tokens (including EOS '.'): 23492\n",
            "2020-09-03 19:48:22 WARNING  Token-level (P)PPL: 68204.57904071093\n",
            "2020-09-03 19:48:22 WARNING  Word-normalized (P)PPL: 11021210.647441747\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 19:48:25.107675: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 19:48:32 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 19:48:32 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 19:48:32 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 19:48:32 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 19:48:36 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
            "2020-09-03 19:48:36 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 19:48:36 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 19:48:36 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 19:48:36 INFO     loading file None\n",
            "2020-09-03 19:48:36 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 19:48:36 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 19:48:36 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 19:48:40 WARNING  # sentences: 100\n",
            "2020-09-03 19:48:40 INFO     FixedBucketSampler:\n",
            "  sample_num=16386, batch_num=36\n",
            "  key=[163, 164, 165, 166, 167, 168, 169, 170, 171]\n",
            "  cnt=[161, 1944, 5868, 5084, 1485, 166, 668, 672, 338]\n",
            "  batch_size=[524, 521, 518, 515, 511, 508, 505, 502, 500]\n",
            "[19:48:40] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 19:50:37 INFO     8931 sents of 16386, batch 20 of 36\n",
            "2020-09-03 19:52:15 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 19:52:15 WARNING  # words (excluding EOS '.'): 12831\n",
            "2020-09-03 19:52:15 WARNING  longest sentence: 130\n",
            "2020-09-03 19:52:15 WARNING  # tokens (including EOS '.'): 16386\n",
            "2020-09-03 19:52:15 WARNING  Token-level (P)PPL: 83969.48826391746\n",
            "2020-09-03 19:52:15 WARNING  Word-normalized (P)PPL: 1942742.167860801\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 19:52:18.165841: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 19:52:25 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 19:52:25 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 19:52:25 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 19:52:25 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 19:52:29 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
            "2020-09-03 19:52:29 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 19:52:29 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 19:52:29 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 19:52:29 INFO     loading file None\n",
            "2020-09-03 19:52:29 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 19:52:29 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 19:52:30 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 19:52:33 WARNING  # sentences: 100\n",
            "2020-09-03 19:52:34 INFO     FixedBucketSampler:\n",
            "  sample_num=14610, batch_num=32\n",
            "  key=[146, 147, 148, 149, 150]\n",
            "  cnt=[144, 3480, 6278, 4116, 592]\n",
            "  batch_size=[513, 510, 506, 503, 500]\n",
            "[19:52:34] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 19:54:20 INFO     9262 sents of 14610, batch 20 of 32\n",
            "2020-09-03 19:55:21 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 19:55:21 WARNING  # words (excluding EOS '.'): 10547\n",
            "2020-09-03 19:55:21 WARNING  longest sentence: 106\n",
            "2020-09-03 19:55:21 WARNING  # tokens (including EOS '.'): 14610\n",
            "2020-09-03 19:55:21 WARNING  Token-level (P)PPL: 63933.93697933221\n",
            "2020-09-03 19:55:21 WARNING  Word-normalized (P)PPL: 4539770.438145941\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 19:55:24.422011: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 19:55:31 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 19:55:31 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 19:55:31 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 19:55:31 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 19:55:36 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
            "2020-09-03 19:55:36 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 19:55:36 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 19:55:36 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 19:55:36 INFO     loading file None\n",
            "2020-09-03 19:55:36 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 19:55:36 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 19:55:36 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 19:55:40 WARNING  # sentences: 100\n",
            "2020-09-03 19:55:40 INFO     FixedBucketSampler:\n",
            "  sample_num=26859, batch_num=55\n",
            "  key=[269, 270, 271, 272]\n",
            "  cnt=[1869, 10452, 11298, 3240]\n",
            "  batch_size=[505, 503, 501, 500]\n",
            "[19:55:40] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 19:59:21 INFO     9753 sents of 26859, batch 20 of 55\n",
            "2020-09-03 20:03:10 INFO     19568 sents of 26859, batch 40 of 55\n",
            "2020-09-03 20:06:00 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 20:06:00 WARNING  # words (excluding EOS '.'): 20759\n",
            "2020-09-03 20:06:00 WARNING  longest sentence: 208\n",
            "2020-09-03 20:06:00 WARNING  # tokens (including EOS '.'): 26859\n",
            "2020-09-03 20:06:00 WARNING  Token-level (P)PPL: 69579.63983869273\n",
            "2020-09-03 20:06:00 WARNING  Word-normalized (P)PPL: 1842629.688556529\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 20:06:03.479854: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 20:06:10 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 20:06:10 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 20:06:10 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 20:06:10 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 20:06:15 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias']\n",
            "2020-09-03 20:06:15 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 20:06:15 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 20:06:15 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 20:06:15 INFO     loading file None\n",
            "2020-09-03 20:06:15 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 20:06:15 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 20:06:15 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 20:06:19 WARNING  # sentences: 100\n",
            "2020-09-03 20:06:20 INFO     FixedBucketSampler:\n",
            "  sample_num=21321, batch_num=45\n",
            "  key=[213, 214, 215, 216, 217, 218]\n",
            "  cnt=[633, 4664, 7881, 5992, 1935, 216]\n",
            "  batch_size=[511, 509, 506, 504, 502, 500]\n",
            "[20:06:20] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n",
            "2020-09-03 20:09:08 INFO     9661 sents of 21321, batch 20 of 45\n",
            "2020-09-03 20:12:01 INFO     19587 sents of 21321, batch 40 of 45\n",
            "2020-09-03 20:12:31 WARNING  Adding EOSes '.' to (P)PPL computation\n",
            "2020-09-03 20:12:31 WARNING  # words (excluding EOS '.'): 16632\n",
            "2020-09-03 20:12:31 WARNING  longest sentence: 167\n",
            "2020-09-03 20:12:31 WARNING  # tokens (including EOS '.'): 21321\n",
            "2020-09-03 20:12:31 WARNING  Token-level (P)PPL: 92082.17056067099\n",
            "2020-09-03 20:12:31 WARNING  Word-normalized (P)PPL: 2310509.31372397\n",
            "/usr/local/lib/python3.6/dist-packages/mxnet/optimizer/optimizer.py:167: UserWarning: WARNING: New optimizer gluonnlp.optimizer.lamb.LAMB is overriding existing optimizer mxnet.optimizer.optimizer.LAMB\n",
            "  Optimizer.opt_registry[name].__name__))\n",
            "2020-09-03 20:12:35.066101: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "2020-09-03 20:12:42 WARNING  Model 'bert-base-polish-uncased-v1' not recognized as an MXNet model; treating as PyTorch model\n",
            "2020-09-03 20:12:42 INFO     loading configuration file bert-base-polish-uncased-v1/config.json\n",
            "2020-09-03 20:12:42 INFO     Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 60000\n",
            "}\n",
            "\n",
            "2020-09-03 20:12:42 INFO     loading weights file bert-base-polish-uncased-v1/pytorch_model.bin\n",
            "2020-09-03 20:12:46 INFO     Weights of BertForMaskedLMOptimized not initialized from pretrained model: ['cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "2020-09-03 20:12:46 INFO     Model name 'bert-base-polish-uncased-v1' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'bert-base-polish-uncased-v1' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "2020-09-03 20:12:46 INFO     Didn't find file bert-base-polish-uncased-v1/added_tokens.json. We won't load it.\n",
            "2020-09-03 20:12:46 INFO     loading file bert-base-polish-uncased-v1/vocab.txt\n",
            "2020-09-03 20:12:46 INFO     loading file None\n",
            "2020-09-03 20:12:46 INFO     loading file bert-base-polish-uncased-v1/special_tokens_map.json\n",
            "2020-09-03 20:12:46 INFO     loading file bert-base-polish-uncased-v1/tokenizer_config.json\n",
            "2020-09-03 20:12:46 WARNING  Language was set but this model does not use language embeddings!\n",
            "2020-09-03 20:12:50 WARNING  # sentences: 100\n",
            "2020-09-03 20:12:51 INFO     FixedBucketSampler:\n",
            "  sample_num=28297, batch_num=58\n",
            "  key=[283, 284, 285, 286, 287]\n",
            "  cnt=[2529, 7050, 9622, 6816, 2280]\n",
            "  batch_size=[507, 505, 503, 501, 500]\n",
            "[20:12:51] src/storage/storage.cc:110: Using GPUPooledRoundedStorageManager.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsqPWmtmhREr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "fe8a36cf-fbb8-4bee-a67a-e2d8978d89f6"
      },
      "source": [
        "!cat output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-1626.3792247772217\n",
            "-1615.1087942123413\n",
            "-1617.0448637008667\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}